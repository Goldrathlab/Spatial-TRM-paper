{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imageio opencv-python alphashape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import alphashape\n",
    "import geopandas as gpd\n",
    "import imageio as io\n",
    "import shapely.affinity as sa\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to overlay Transcripts, Aligned IF, Xenium DAPI, H and E, cell segmentation, transcripts/cell masks colored by cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adata \"finalized_adata\" that contains cells from Day 8 rep 2\n",
    "experiment_name = \"day8_SI_r2\"\n",
    "whole_adata = sc.read(\"../data/adata/timecourse.h5ad\")\n",
    "finalized_adata = whole_adata[whole_adata.obs.batch == experiment_name]\n",
    "\n",
    "# the following has the transcripts saved. It is a temporary adata along the processing pipeline\n",
    "path_to_adata_with_transcripts = \"../data/adata/day8_r2_with_transcripts.h5ad\"\n",
    "\n",
    "# all h and e and IF are generated and saved\n",
    "path_to_h_and_e = \"../data/images/day8_r2_h_and_e_alignment_gan.npy\"\n",
    "path_to_if = \"../data/images/day8_r2_IF_alignment.npy\"\n",
    "\n",
    "xenium_output_path = \"../data/xenium_output/day8_r2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_image(path: str):\n",
    "    \"\"\"\n",
    "    Import the max projected DAPI staining from the provided xenium output folder\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the xenium folder\n",
    "\n",
    "    Returns:\n",
    "        img (np.array): image as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    file = os.path.join(path, \"morphology_mip.ome.tif\")\n",
    "    img = io.imread(file)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_pixel_size(path: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the pixel size for micron to pixel transform from the provided xenium output folder\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the xenium folder\n",
    "\n",
    "    Returns:\n",
    "        pixel_size (float): pixel size in microns\n",
    "    \"\"\"\n",
    "    file = open(os.path.join(path, \"experiment.xenium\"))\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment[\"pixel_size\"]\n",
    "    return pixel_size\n",
    "\n",
    "\n",
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    \"\"\"\n",
    "    Create a cell boundary with alpha shape from the provided points\n",
    "\n",
    "    Args:\n",
    "        points (pd.DataFrame): dataframe with columns \"x\" and \"y\" for the positions of the transcripts\n",
    "        alpha (float): alpha value for the alpha shape\n",
    "\n",
    "    Returns:\n",
    "        shape (shapely.geometry.Polygon): alpha shape cell segmentation boundary\n",
    "    \"\"\"\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "\n",
    "# Function to generate a random color in RGB format\n",
    "def random_color():\n",
    "    \"\"\"\n",
    "    Generate a random color in RGB format\n",
    "\n",
    "    Returns:\n",
    "        color (str): color in RGB format\n",
    "    \"\"\"\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(\n",
    "        random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the DAPI, IF, and H&E images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in H&E, DAPI, and IF images\n",
    "xenium_dapi = import_image(xenium_output_path)\n",
    "\n",
    "try:\n",
    "    IF_image = np.load(path_to_if)\n",
    "except:\n",
    "    print(\"No IF for this experiment\")\n",
    "    IF_image = xenium_dapi\n",
    "\n",
    "try:\n",
    "    h_an_e = np.load(path_to_h_and_e)\n",
    "except:\n",
    "    print(\"No H&E for this experiment\")\n",
    "    h_an_e = xenium_dapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the adata with the transcript positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the adata holding the transcripts\n",
    "transcripts = sc.read(path_to_adata_with_transcripts)\n",
    "points = transcripts.uns[\"points\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the different parts of the transcripts df for fast indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the different parts of the transcripts df for fast indexing\n",
    "points_x = points.x.values\n",
    "points_y = points.y.values\n",
    "points_z = points.z.values\n",
    "points_gene = points.gene.values\n",
    "points_cell = points.cell.values\n",
    "points_split_cell = points.split_cell.values\n",
    "points[\"split_cell\"] = points[\"split_cell\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform the transcript coordinates from microns to pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the transcript coordinates from microns to pixels\n",
    "pixel_size = get_pixel_size(xenium_output_path)\n",
    "transformed_x = points_x * (1 / pixel_size)\n",
    "transformed_y = points_y * (1 / pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling the dapi overview by 50x (you can plot the thumbnail to figure out what region you want to zoom in on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscaling the dapi overview by 50x (you can plot the thumbnail to figure out what region you want to zoom in on)\n",
    "\n",
    "down_factor = 50\n",
    "\n",
    "new_width = int(xenium_dapi.shape[1] / down_factor)\n",
    "new_height = int(xenium_dapi.shape[0] / down_factor)\n",
    "\n",
    "thumbnail = cv2.resize(xenium_dapi, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking a bounding rectangle to zoom in on from the thumbnail image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 555\n",
    "max_y = 600\n",
    "\n",
    "min_x = 368\n",
    "max_x = 379"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the transcripts falling in the box you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transcripts falling in the box you created\n",
    "\n",
    "min_x = min_x * down_factor\n",
    "min_y = min_y * down_factor\n",
    "max_x = max_x * down_factor\n",
    "max_y = max_y * down_factor\n",
    "\n",
    "\n",
    "subsetted_indices = np.where(\n",
    "    (transformed_x > min_y)\n",
    "    & (transformed_x < max_y)\n",
    "    & (transformed_y > min_x)\n",
    "    & (transformed_y < max_x)\n",
    ")[0]\n",
    "\n",
    "transcripts_df = pd.DataFrame(\n",
    "    zip(\n",
    "        transformed_x[subsetted_indices],\n",
    "        transformed_y[subsetted_indices],\n",
    "        points_gene[subsetted_indices],\n",
    "        points_split_cell[subsetted_indices],\n",
    "    ),\n",
    "    index=points_cell[subsetted_indices],\n",
    "    columns=[\"x\", \"y\", \"gene\", \"split_cell\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the data has been prepared, plot each panel one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1 - Plotting a whole tissue view of H&E staining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsize by 4 to speed up plotting with minimal resolution loss\n",
    "plot_down = 4\n",
    "\n",
    "# Load in the H&E image\n",
    "thumbnail = cv2.resize(\n",
    "    h_an_e, (np.shape(h_an_e)[0] // plot_down, np.shape(h_an_e)[1] // plot_down)\n",
    ")\n",
    "# Define the RGB value for black\n",
    "black_color = [0, 0, 0]\n",
    "\n",
    "# Create a mask for black pixels\n",
    "black_pixels = np.all(thumbnail[:, :, :3] == black_color, axis=-1)\n",
    "\n",
    "# Replace black pixels with white\n",
    "thumbnail[black_pixels] = [255, 255, 255]\n",
    "\n",
    "\n",
    "# Plot the large H&E staining\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax0 = plt.gca()\n",
    "# 'thumbnail' is the image data\n",
    "ax0.imshow(thumbnail)\n",
    "ax0.set_xlim(300, np.shape(thumbnail)[1])\n",
    "ax0.set_ylim(np.shape(thumbnail)[0], 400)\n",
    "\n",
    "# Add a black rectangle\n",
    "rectangle = Rectangle(\n",
    "    (min_y // plot_down, min_x // plot_down),\n",
    "    max_y // plot_down - min_y // plot_down,\n",
    "    max_x // plot_down - min_x // plot_down,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "ax0.add_patch(rectangle)\n",
    "ax0.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a second box to zoom in on for an even closer zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a second zoom in box coordinates\n",
    "second_min_y = 1440\n",
    "second_max_y = 1600\n",
    "\n",
    "second_min_x = 150\n",
    "second_max_x = 450\n",
    "\n",
    "# Adjusting coordinates based on first window zoom\n",
    "side1 = min_y + second_min_y\n",
    "side2 = max_y - (max_y - (min_y + second_max_y))\n",
    "side3 = second_min_x + min_x\n",
    "side4 = max_x - (max_x - (second_max_x + min_x))\n",
    "\n",
    "# Figuring out which coordinates of transcripts lie in the second box\n",
    "subsetted_indices_second = np.where(\n",
    "    (transformed_x > side1)\n",
    "    & (transformed_x < side2)\n",
    "    & (transformed_y > side3)\n",
    "    & (transformed_y < side4)\n",
    ")[0]\n",
    "\n",
    "# Subsetting the transcripts df\n",
    "transcripts_df_second = pd.DataFrame(\n",
    "    zip(\n",
    "        transformed_x[subsetted_indices_second],\n",
    "        transformed_y[subsetted_indices_second],\n",
    "        points_gene[subsetted_indices_second],\n",
    "        points_split_cell[subsetted_indices_second],\n",
    "    ),\n",
    "    index=points_cell[subsetted_indices_second],\n",
    "    columns=[\"x\", \"y\", \"gene\", \"split_cell\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2 - Plotting zoomed in H&E staining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "ax3 = plt.gca()\n",
    "img_cropped = h_an_e[min_x:max_x, min_y:max_y]\n",
    "ax3.imshow(img_cropped)\n",
    "\n",
    "# Add a black rectangle\n",
    "rectangle = Rectangle(\n",
    "    (second_min_y, second_min_x),\n",
    "    second_max_y - second_min_y,\n",
    "    second_max_x - second_min_x,\n",
    "    linewidth=4,\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "ax3.add_patch(rectangle)\n",
    "ax3.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3 - Plotting zoomed in IF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "ax4 = plt.gca()\n",
    "\n",
    "# Specify the IF channels that you want to plot\n",
    "if_channels = [2, 1]\n",
    "\n",
    "mapped_ims = []\n",
    "for g in range(len(if_channels)):\n",
    "    # Grab the current IF channel\n",
    "    image = IF_image[min_x:max_x, min_y:max_y, if_channels[g]]\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "\n",
    "    normalized_image = (image - min_val) / (max_val - min_val)\n",
    "\n",
    "    # If the channel is CD8A, perform a top hat and black hat transform\n",
    "    if if_channels[g] == 2:\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 30))\n",
    "        # Top Hat Transform\n",
    "        topHat = cv2.morphologyEx(normalized_image, cv2.MORPH_TOPHAT, kernel)\n",
    "        # Black Hat Transform\n",
    "        blackHat = cv2.morphologyEx(normalized_image, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "        normalized_image = normalized_image + topHat - blackHat\n",
    "\n",
    "        normalized_image = normalized_image * 2\n",
    "\n",
    "    mapped_ims.append(normalized_image)\n",
    "\n",
    "# Add a last blank channel\n",
    "mapped_ims.append(\n",
    "    np.zeros(np.shape(IF_image[min_x:max_x, min_y:max_y, if_channels[g]]))\n",
    ")\n",
    "\n",
    "full_im = np.dstack(mapped_ims)\n",
    "ax4.imshow(full_im)\n",
    "\n",
    "# Add a black rectangle\n",
    "rectangle2 = Rectangle(\n",
    "    (second_min_y, second_min_x),\n",
    "    second_max_y - second_min_y,\n",
    "    second_max_x - second_min_x,\n",
    "    linewidth=4,\n",
    "    edgecolor=\"white\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "\n",
    "ax4.add_patch(rectangle2)\n",
    "ax4.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4 - Plotting cell segmentation boundaries over the zoomed-in region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "ax1 = plt.gca()\n",
    "\n",
    "# Color cells by leiden\n",
    "segmentation_face_color = \"leiden\"\n",
    "inside_alpha = 0.34\n",
    "outside_alpha = 0.34\n",
    "\n",
    "# Add the key of facecolor of the cells to each transcript row\n",
    "celltypes = []\n",
    "ids = np.array([i.split(\"_\")[-1] for i in finalized_adata.obs.index.values]).astype(int)\n",
    "id_df = pd.DataFrame(\n",
    "    zip(ids, finalized_adata.obs[segmentation_face_color].values),\n",
    "    columns=[\"id\", segmentation_face_color],\n",
    ")\n",
    "transcripts_with_obs = transcripts_df.merge(\n",
    "    id_df, left_on=\"split_cell\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "transcripts_with_obs = transcripts_with_obs.dropna(axis=0)\n",
    "\n",
    "# Group transcripts by their Baysor assignment\n",
    "print(\"Making Shapes\")\n",
    "gby = transcripts_with_obs[\n",
    "    (transcripts_with_obs.split_cell != 0) & (transcripts_with_obs.split_cell != -1)\n",
    "].groupby(\"split_cell\")\n",
    "\n",
    "# Create a cell segmentation boundary for each set of transcripts, and get the color of the mask\n",
    "shapes = []\n",
    "for group in tqdm(gby):\n",
    "    shapes.append(make_alphashape(group[1][[\"x\", \"y\"]].values, alpha=0.05))\n",
    "    ctype = group[1][segmentation_face_color].values[0]\n",
    "    cell_location = np.where(\n",
    "        finalized_adata.obs[segmentation_face_color].cat.categories == ctype\n",
    "    )[0]\n",
    "    try:\n",
    "        celltypes.append(\n",
    "            finalized_adata.uns[f\"{segmentation_face_color}_colors\"][cell_location][0]\n",
    "        )\n",
    "    except:\n",
    "        celltypes.append(\n",
    "            finalized_adata.uns[f\"{segmentation_face_color}_colors\"][cell_location[0]]\n",
    "        )\n",
    "shapes = gpd.GeoSeries(shapes)\n",
    "colors = celltypes\n",
    "\n",
    "# Display the Xenium DAPI image\n",
    "img_cropped = xenium_dapi[\n",
    "    min_x:max_x, min_y:max_y\n",
    "]  # [second_min_x:second_max_x, second_min_y:second_max_y]\n",
    "ax1.imshow(img_cropped, vmax=np.percentile(img_cropped, 99.9), cmap=\"Greys_r\")\n",
    "\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes:\n",
    "    scaled_polygon = sa.translate(original_polygon, -min_y, -min_x)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "\n",
    "adjusted_shapes = gpd.GeoSeries(adjusted_shapes)\n",
    "\n",
    "for geometry, color in zip(adjusted_shapes, colors):\n",
    "    if geometry.geom_type == \"Polygon\":\n",
    "        patch = plt.Polygon(\n",
    "            list(zip(*geometry.exterior.xy)),\n",
    "            facecolor=color,\n",
    "            edgecolor=\"none\",\n",
    "            alpha=inside_alpha,\n",
    "            zorder=1,\n",
    "        )\n",
    "        ax1.add_patch(patch)\n",
    "    elif geometry.geom_type == \"MultiPolygon\":\n",
    "        for poly in geometry:\n",
    "            patch = plt.Polygon(\n",
    "                list(zip(*poly.exterior.xy)),\n",
    "                facecolor=color,\n",
    "                edgecolor=\"none\",\n",
    "                alpha=inside_alpha,\n",
    "                zorder=1,\n",
    "            )\n",
    "            ax1.add_patch(patch)\n",
    "\n",
    "# Plot polygon edges with edgecolor based on data values\n",
    "for geometry, color in zip(adjusted_shapes, colors):\n",
    "    if geometry.geom_type == \"Polygon\":\n",
    "        ax1.plot(*geometry.exterior.xy, color=color, alpha=outside_alpha)\n",
    "    elif geometry.geom_type == \"MultiPolygon\":\n",
    "        for poly in geometry:\n",
    "            ax1.plot(*poly.exterior.xy, color=color, alpha=outside_alpha)\n",
    "\n",
    "\n",
    "rectangle2 = Rectangle(\n",
    "    (second_min_y, second_min_x),\n",
    "    second_max_y - second_min_y,\n",
    "    second_max_x - second_min_x,\n",
    "    linewidth=4,\n",
    "    edgecolor=\"white\",\n",
    "    facecolor=\"none\",\n",
    "    zorder=2,\n",
    ")\n",
    "ax1.add_patch(rectangle2)\n",
    "ax1.set_xlim(0, max_y - min_y)\n",
    "ax1.set_ylim(0, max_x - min_x)\n",
    "ax1.invert_yaxis()\n",
    "# ax1.axis('equal')\n",
    "ax1.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 5 - Plotting the transcript positions over the even further zoomed-in region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 10), dpi=300)\n",
    "ax1 = plt.gca()\n",
    "\n",
    "# Color cells by leiden\n",
    "segmentation_face_color = \"leiden\"\n",
    "inside_alpha = 0.34\n",
    "outside_alpha = 0.8\n",
    "\n",
    "# Add the key of facecolor of the cells to each transcript row\n",
    "celltypes = []\n",
    "ids = np.array([i.split(\"_\")[-1] for i in finalized_adata.obs.index.values]).astype(int)\n",
    "id_df = pd.DataFrame(\n",
    "    zip(ids, finalized_adata.obs[segmentation_face_color].values),\n",
    "    columns=[\"id\", segmentation_face_color],\n",
    ")\n",
    "transcripts_with_obs = transcripts_df_second.merge(\n",
    "    id_df, left_on=\"split_cell\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "transcripts_with_obs = transcripts_with_obs.dropna(axis=0)\n",
    "\n",
    "# Group transcripts by their Baysor assignment\n",
    "print(\"Making Shapes\")\n",
    "gby = transcripts_with_obs[\n",
    "    (transcripts_with_obs.split_cell != 0) & (transcripts_with_obs.split_cell != -1)\n",
    "].groupby(\"split_cell\")\n",
    "\n",
    "# Create a cell segmentation boundary for each set of transcripts, and get the color of the mask\n",
    "shapes = []\n",
    "for group in tqdm(gby):\n",
    "    shapes.append(make_alphashape(group[1][[\"x\", \"y\"]].values, alpha=0.05))\n",
    "    ctype = group[1][segmentation_face_color].values[0]\n",
    "    cell_location = np.where(\n",
    "        finalized_adata.obs[segmentation_face_color].cat.categories == ctype\n",
    "    )[0]\n",
    "    try:\n",
    "        celltypes.append(\n",
    "            finalized_adata.uns[f\"{segmentation_face_color}_colors\"][cell_location][0]\n",
    "        )\n",
    "    except:\n",
    "        celltypes.append(\n",
    "            finalized_adata.uns[f\"{segmentation_face_color}_colors\"][cell_location[0]]\n",
    "        )\n",
    "shapes = gpd.GeoSeries(shapes)\n",
    "colors = [\"#D3D3D3\" for s in range(len(shapes))]\n",
    "\n",
    "\n",
    "# Display the Xenium DAPI image\n",
    "img_cropped = xenium_dapi[min_x:max_x, min_y:max_y][\n",
    "    second_min_x:second_max_x, second_min_y:second_max_y\n",
    "]\n",
    "\n",
    "ax1.imshow(\n",
    "    img_cropped,\n",
    "    vmax=np.percentile(img_cropped, 99.9),\n",
    "    vmin=np.percentile(img_cropped, 30),\n",
    "    cmap=\"Greys_r\",\n",
    ")\n",
    "\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes:\n",
    "    scaled_polygon = sa.translate(\n",
    "        original_polygon, -min_y - second_min_y, -min_x - second_min_x\n",
    "    )\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "\n",
    "adjusted_shapes = gpd.GeoSeries(adjusted_shapes)\n",
    "\n",
    "# Plot polygons with facecolor based on data values\n",
    "for geometry, color in zip(adjusted_shapes, colors):\n",
    "    if geometry.geom_type == \"Polygon\":\n",
    "        patch = plt.Polygon(\n",
    "            list(zip(*geometry.exterior.xy)),\n",
    "            facecolor=color,\n",
    "            edgecolor=\"none\",\n",
    "            alpha=inside_alpha,\n",
    "            zorder=1,\n",
    "        )\n",
    "        ax1.add_patch(patch)\n",
    "    elif geometry.geom_type == \"MultiPolygon\":\n",
    "        for poly in geometry:\n",
    "            patch = plt.Polygon(\n",
    "                list(zip(*poly.exterior.xy)),\n",
    "                facecolor=color,\n",
    "                edgecolor=\"none\",\n",
    "                alpha=inside_alpha,\n",
    "                zorder=1,\n",
    "            )\n",
    "            ax1.add_patch(patch)\n",
    "\n",
    "# Plot polygon edges with edgecolor based on data values\n",
    "for geometry, color in zip(adjusted_shapes, colors):\n",
    "    if geometry.geom_type == \"Polygon\":\n",
    "        ax1.plot(*geometry.exterior.xy, color=color, linewidth=4, alpha=outside_alpha)\n",
    "    elif geometry.geom_type == \"MultiPolygon\":\n",
    "        for poly in geometry:\n",
    "            ax1.plot(*poly.exterior.xy, color=color, linewidth=4, alpha=outside_alpha)\n",
    "\n",
    "transcripts_genes_only = transcripts_df_second\n",
    "\n",
    "# Get a random color for each transcript type\n",
    "transcript_colors = [random_color() for _ in range(500)]\n",
    "pt_size = 1.2\n",
    "gene_subset = finalized_adata.var.index.values\n",
    "\n",
    "# Plot each transcript in the FOV\n",
    "col_ct = 0\n",
    "for i in gene_subset:\n",
    "    transcripts_genes_only_current = transcripts_genes_only[\n",
    "        transcripts_genes_only[\"gene\"] == i\n",
    "    ]\n",
    "    for x, y in zip(\n",
    "        transcripts_genes_only_current.x.values, transcripts_genes_only_current.y.values\n",
    "    ):\n",
    "        circle = patches.Circle(\n",
    "            (x - (min_y + second_min_y), y - (min_x + second_min_x)),\n",
    "            radius=pt_size,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.01,\n",
    "            facecolor=transcript_colors[col_ct],\n",
    "            alpha=1,\n",
    "            zorder=2,\n",
    "        )\n",
    "        ax1.add_patch(circle)\n",
    "    col_ct += 1\n",
    "\n",
    "col_ct = 0\n",
    "for i in gene_subset:\n",
    "    plt.scatter([], [], c=transcript_colors[col_ct], label=i)\n",
    "    col_ct += 1\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "ax1.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 6 - Plotting select transcripts over IF staining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 10), dpi=300)\n",
    "ax1 = plt.gca()\n",
    "\n",
    "# Add the key of facecolor of the cells to each transcript row\n",
    "celltypes = []\n",
    "ids = np.array([i.split(\"_\")[-1] for i in finalized_adata.obs.index.values]).astype(int)\n",
    "id_df = pd.DataFrame(\n",
    "    zip(ids, finalized_adata.obs[segmentation_face_color].values),\n",
    "    columns=[\"id\", segmentation_face_color],\n",
    ")\n",
    "transcripts_with_obs = transcripts_df_second.merge(\n",
    "    id_df, left_on=\"split_cell\", right_on=\"id\", how=\"left\"\n",
    ")\n",
    "transcripts_with_obs = transcripts_with_obs.dropna(axis=0)\n",
    "\n",
    "\n",
    "# Define which IF channels to plot\n",
    "if_channels = [2, 1]\n",
    "\n",
    "# Normalize CD8a channel of IF image\n",
    "mapped_ims = []\n",
    "for g in range(len(if_channels)):\n",
    "    image = IF_image[min_x:max_x, min_y:max_y, if_channels[g]][\n",
    "        second_min_x:second_max_x, second_min_y:second_max_y\n",
    "    ]\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "\n",
    "    normalized_image = (image - min_val) / (max_val - min_val)\n",
    "\n",
    "    if if_channels[g] == 2:\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50, 50))\n",
    "        # Top Hat Transform\n",
    "        topHat = cv2.morphologyEx(normalized_image, cv2.MORPH_TOPHAT, kernel)\n",
    "        # Black Hat Transform\n",
    "        blackHat = cv2.morphologyEx(normalized_image, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "        normalized_image = normalized_image + topHat - blackHat\n",
    "\n",
    "        normalized_image = normalized_image**2\n",
    "\n",
    "    mapped_ims.append(normalized_image)\n",
    "\n",
    "mapped_ims.append(\n",
    "    np.zeros(\n",
    "        np.shape(\n",
    "            IF_image[min_x:max_x, min_y:max_y, if_channels[g]][\n",
    "                second_min_x:second_max_x, second_min_y:second_max_y\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the IF image\n",
    "full_im = np.dstack(mapped_ims)\n",
    "\n",
    "img_cropped = full_im\n",
    "ax1.imshow(\n",
    "    img_cropped,\n",
    "    vmax=np.percentile(img_cropped, 99.9),\n",
    "    vmin=np.percentile(img_cropped, 30),\n",
    "    cmap=\"Greys_r\",\n",
    ")\n",
    "\n",
    "transcripts_genes_only = transcripts_df_second\n",
    "\n",
    "# Plot select transcripts by the specified colors\n",
    "xist = \"#00FFFF\"\n",
    "cd8a = \"#FEFDFD\"  # White\n",
    "cd8b1 = \"#a7a7a7\"  # Bright Red\n",
    "gzmb = \"#FF00FF\"\n",
    "\n",
    "transcript_colors = [cd8a, cd8b1, gzmb, xist]\n",
    "\n",
    "pt_size = 1.5\n",
    "gene_subset = [\"Cd8a\", \"Cd8b1\", \"Gzmb\", \"Xist\"]\n",
    "col_ct = 0\n",
    "for i in gene_subset:\n",
    "    transcripts_genes_only_current = transcripts_genes_only[\n",
    "        transcripts_genes_only[\"gene\"] == i\n",
    "    ]\n",
    "    for x, y in zip(\n",
    "        transcripts_genes_only_current.x.values, transcripts_genes_only_current.y.values\n",
    "    ):\n",
    "        circle = patches.Circle(\n",
    "            (x - (min_y + second_min_y), y - (min_x + second_min_x)),\n",
    "            radius=pt_size,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.01,\n",
    "            facecolor=transcript_colors[col_ct],\n",
    "            alpha=1,\n",
    "            zorder=2,\n",
    "        )\n",
    "        ax1.add_patch(circle)\n",
    "    col_ct += 1\n",
    "\n",
    "col_ct = 0\n",
    "for i in gene_subset:\n",
    "    plt.scatter([], [], c=transcript_colors[col_ct], label=i)\n",
    "    col_ct += 1\n",
    "\n",
    "ax1.axis(\"off\")\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merscope_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
