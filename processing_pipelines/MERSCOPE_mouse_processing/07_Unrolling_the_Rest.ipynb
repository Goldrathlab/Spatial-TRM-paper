{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely\n",
    "import glob\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from core_functions.unrolling import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Training model on reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_path = \"D:/amonell/merscope_final/SI-Ctrl-L-RAR-R-dist-1-VS120-NP_Beta8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_topics = np.load(os.path.join(reference_path, \"unrolling\", \"base_topicas.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:/amonell/merscope_final\"\n",
    "path_adata = [\n",
    "    i\n",
    "    for i in glob.glob(os.path.join(data_dir, \"SI-*\"))\n",
    "    if os.path.basename(i) == os.path.basename(reference_path)\n",
    "][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Prepare Reference Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(os.path.join(path_adata, \"adatas\", \"05_reference_unrolled.h5ad\"))\n",
    "all_spatial = adata.obsm[\"X_spatial\"]\n",
    "\n",
    "try:\n",
    "    adata.X = adata.X.A\n",
    "except:\n",
    "    print(\"Adata already in array format\")\n",
    "\n",
    "topics_contain = [\"1\", \"2\"]\n",
    "spatial_points = np.array(\n",
    "    adata[adata.obs[\"topic\"].isin(topics_contain), :].obsm[\"X_spatial\"]\n",
    ")\n",
    "other_spatial = np.array(\n",
    "    adata[~adata.obs[\"topic\"].isin(topics_contain), :].obsm[\"X_spatial\"]\n",
    ")\n",
    "\n",
    "spatial_points = remove_outliers(spatial_points, 99)\n",
    "\n",
    "downsize = 10\n",
    "base_image = create_base_image(spatial_points, other_spatial, downsize=downsize)\n",
    "\n",
    "file_path = os.path.join(path_adata, \"unrolling\", \"roll_image_before_model.png\")\n",
    "base_image.save(file_path)\n",
    "adata.uns[\"unrolling_downsize\"] = downsize\n",
    "adata.write(os.path.join(path_adata, \"adatas\", \"05_reference_unrolled.h5ad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Parse Reference Image Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_num_points = 100000\n",
    "json_file_path = os.path.join(path_adata, \"unrolling\", \"roll_image_before_model.json\")\n",
    "removals, points, top_points, mid_points = extract_json_info(json_file_path)\n",
    "dont_remove, index_set = get_removal_indices(adata, removals, all_spatial)\n",
    "(\n",
    "    x_points_bottom,\n",
    "    y_points_bottom,\n",
    "    x_points_mid,\n",
    "    y_points_mid,\n",
    "    x_points_top,\n",
    "    y_points_top,\n",
    ") = identify_spiral(adata, points, top_points, mid_points, base_num_points)\n",
    "(\n",
    "    all_points,\n",
    "    distances_top,\n",
    "    indices_top,\n",
    "    distances_bottom,\n",
    "    indices_bottom,\n",
    "    distances_mid,\n",
    "    indices_mid,\n",
    ") = get_distances_and_indices(\n",
    "    adata,\n",
    "    dont_remove,\n",
    "    x_points_bottom,\n",
    "    y_points_bottom,\n",
    "    x_points_mid,\n",
    "    y_points_mid,\n",
    "    x_points_top,\n",
    "    y_points_top,\n",
    "    base_num_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Get Ready to train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "##### Prepare Training Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_presorted = (\n",
    "    sc.read(os.path.join(path_adata, \"adatas\", \"04_tissue_cleared.h5ad\"))\n",
    "    .obs[\"topic\"]\n",
    "    .values[dont_remove]\n",
    ")\n",
    "model_input = np.array(\n",
    "    [\n",
    "        stats.zscore(distances_top),\n",
    "        stats.zscore(distances_mid),\n",
    "        stats.zscore(distances_bottom),\n",
    "        indices_top,\n",
    "        indices_mid,\n",
    "        indices_bottom,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Prepare Training Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = adata.obs[\"longitudinal\"].values[dont_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### Define and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_continuous_features = np.shape(model_input)[0]  # Number of continuous features\n",
    "unique_categories = np.unique(topic_presorted)\n",
    "embedding_dim = 3  # Dimensionality of the embedding space\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Categorical data as a 1D array\n",
    "categorical_data = topic_presorted\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_data_encoded = encoder.fit_transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "# Define input for continuous data\n",
    "continuous_input = Input(shape=(num_continuous_features,), name=\"continuous_input\")\n",
    "\n",
    "# Define input for one-hot encoded categorical data\n",
    "categorical_input = Input(\n",
    "    shape=(categorical_data_encoded.shape[1],), name=\"categorical_input\"\n",
    ")\n",
    "\n",
    "# Concatenate the continuous and categorical inputs\n",
    "concatenated_inputs = Concatenate()([continuous_input, categorical_input])\n",
    "\n",
    "# Build the neural network architecture\n",
    "x = Dense(64, activation=\"relu\")(concatenated_inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, activation=\"linear\", name=\"output\")(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[continuous_input, categorical_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\"\n",
    ")  # Use an appropriate loss function for your task\n",
    "\n",
    "continuous_data = model_input.T\n",
    "target_values = axis\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    {\n",
    "        \"continuous_input\": continuous_data,\n",
    "        \"categorical_input\": categorical_data_encoded,\n",
    "    },\n",
    "    y=target_values,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Use model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model for prediction on new data\n",
    "new_continuous_data = continuous_data\n",
    "new_categorical_data_encoded = categorical_data_encoded\n",
    "# predictions = model.predict({'continuous_input': new_continuous_data, 'categorical_input': new_categorical_data_encoded})\n",
    "predictions = model.predict(\n",
    "    {\n",
    "        \"continuous_input\": new_continuous_data,\n",
    "        \"categorical_input\": new_categorical_data_encoded,\n",
    "    }\n",
    ")\n",
    "new_predicts = np.zeros(len(adata.obs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predicts[dont_remove] = predictions.flatten()\n",
    "adata.obs[\"predicted_longitudinal\"] = new_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"spatial\", color=\"predicted_longitudinal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Unrolling the rest of the swiss rolls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Put in the path to the experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:/amonell/merscope_final\"\n",
    "input_folders = glob.glob(os.path.join(data_dir, \"SI-*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path_adata in input_folders:\n",
    "    adata = sc.read(os.path.join(path_adata, \"adatas\", \"04_tissue_cleared.h5ad\"))\n",
    "    all_spatial = adata.obsm[\"X_spatial\"]\n",
    "\n",
    "    try:\n",
    "        adata.X = adata.X.A\n",
    "    except:\n",
    "        print(\"Adata already in array format\")\n",
    "\n",
    "    topics_contain = base_topics\n",
    "    spatial_points = np.array(\n",
    "        adata[adata.obs[\"topic\"].isin(topics_contain), :].obsm[\"X_spatial\"]\n",
    "    )\n",
    "    other_spatial = np.array(\n",
    "        adata[~adata.obs[\"topic\"].isin(topics_contain), :].obsm[\"X_spatial\"]\n",
    "    )\n",
    "\n",
    "    spatial_points = remove_outliers(spatial_points, 99)\n",
    "\n",
    "    downsize = 10\n",
    "    base_image = create_base_image(spatial_points, other_spatial, downsize=downsize)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(os.path.join(path_adata, \"unrolling\"))\n",
    "    except:\n",
    "        print(\"unrolling directory already exists\")\n",
    "    file_path = os.path.join(path_adata, \"unrolling\", \"roll_image_for_prediction.png\")\n",
    "    base_image.save(file_path)\n",
    "    adata.uns[\"unrolling_downsize\"] = downsize\n",
    "    adata.write(os.path.join(path_adata, \"adatas\", \"04_tissue_cleared.h5ad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Go to labelme (pip install labelme > labelme) > open > open roll_image.png > create polygons > make polygons and save. Our labeled rolls are in the ./labels/unrolling folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rolls(json_file_path, adata):\n",
    "    # Load the JSON data from the file\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Extract relevant information from the JSON data\n",
    "    image_height = data[\"imageHeight\"]\n",
    "    image_width = data[\"imageWidth\"]\n",
    "    image_path = data[\"imagePath\"]\n",
    "    shapes = data[\"shapes\"]\n",
    "\n",
    "    unique_rolls = []\n",
    "    for shape in shapes:\n",
    "        label = shape[\"label\"]\n",
    "        if (\"roll\" in label) & (len(label) == 5):\n",
    "            unique_rolls.append(label)\n",
    "\n",
    "    rollers = []\n",
    "    # Process the shapes (annotations)\n",
    "    for roll_name in unique_rolls:\n",
    "        removals = []\n",
    "        bottom_points = []\n",
    "        top_points = []\n",
    "        mid_points = []\n",
    "        mid_points = []\n",
    "        roll_shapes = []\n",
    "        for shape in shapes:\n",
    "            label = shape[\"label\"]\n",
    "            if label == f\"bottom_{roll_name}\":\n",
    "                bottom_points.append(shape[\"points\"])\n",
    "            elif label == f\"top_{roll_name}\":\n",
    "                top_points.append(shape[\"points\"])\n",
    "            elif label == f\"mid_{roll_name}\":\n",
    "                mid_points.append(shape[\"points\"])\n",
    "            elif label == f\"removals_{roll_name}\":\n",
    "                removals.append(shape[\"points\"])\n",
    "            elif label == roll_name:\n",
    "                roll_shapes.append(shape[\"points\"])\n",
    "        rollers.append(\n",
    "            [removals, bottom_points, top_points, mid_points, roll_shapes, roll_name]\n",
    "        )\n",
    "    return rollers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path_adata in input_folders[3:4]:\n",
    "    whole_adata = sc.read(os.path.join(path_adata, \"adatas\", \"04_tissue_cleared.h5ad\"))\n",
    "\n",
    "    json_file_path = os.path.join(\n",
    "        path_adata, \"unrolling\", \"roll_image_for_prediction.json\"\n",
    "    )\n",
    "\n",
    "    roll_counter = 0\n",
    "    rolls = extract_rolls(json_file_path, whole_adata)\n",
    "    roll_names = [i[5] for i in rolls]\n",
    "\n",
    "    for roll in rolls:\n",
    "\n",
    "        base_num_points = 100000\n",
    "        removals = roll[0]\n",
    "        points = roll[1]\n",
    "        top_points = roll[2]\n",
    "        mid_points = roll[3]\n",
    "        roll_shape = np.array(roll[4][0]) * whole_adata.uns[\"unrolling_downsize\"]\n",
    "        roll_shape = shapely.Polygon(roll_shape)\n",
    "\n",
    "        keepers = []\n",
    "        for pt in whole_adata.obsm[\"X_spatial\"]:\n",
    "            if roll_shape.contains(shapely.Point(pt)):\n",
    "                keepers.append(True)\n",
    "            else:\n",
    "                keepers.append(False)\n",
    "\n",
    "        adata = whole_adata[keepers, :]\n",
    "        all_spatial = adata.obsm[\"X_spatial\"]\n",
    "\n",
    "        dont_remove, index_set = get_removal_indices(adata, removals, all_spatial)\n",
    "\n",
    "        (\n",
    "            x_points_bottom,\n",
    "            y_points_bottom,\n",
    "            x_points_mid,\n",
    "            y_points_mid,\n",
    "            x_points_top,\n",
    "            y_points_top,\n",
    "        ) = identify_spiral(adata, points, top_points, mid_points, base_num_points)\n",
    "        (\n",
    "            all_points,\n",
    "            distances_top,\n",
    "            indices_top,\n",
    "            distances_bottom,\n",
    "            indices_bottom,\n",
    "            distances_mid,\n",
    "            indices_mid,\n",
    "        ) = get_distances_and_indices(\n",
    "            adata,\n",
    "            dont_remove,\n",
    "            x_points_bottom,\n",
    "            y_points_bottom,\n",
    "            x_points_mid,\n",
    "            y_points_mid,\n",
    "            x_points_top,\n",
    "            y_points_top,\n",
    "            base_num_points,\n",
    "        )\n",
    "\n",
    "        topic_presorted = adata.obs[\"topic\"].values[dont_remove]\n",
    "        model_input = np.array(\n",
    "            [\n",
    "                stats.zscore(distances_top),\n",
    "                stats.zscore(distances_mid),\n",
    "                stats.zscore(distances_bottom),\n",
    "                indices_top,\n",
    "                indices_mid,\n",
    "                indices_bottom,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Categorical data as a 1D array\n",
    "        categorical_data = topic_presorted\n",
    "\n",
    "        categorical_data_encoded = encoder.transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "        continuous_data = model_input.T\n",
    "\n",
    "        predictions = model.predict(\n",
    "            {\n",
    "                \"continuous_input\": continuous_data,\n",
    "                \"categorical_input\": categorical_data_encoded,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        new_longitidunal = np.zeros(len(adata.obs.index))\n",
    "        new_longitidunal[dont_remove] = predictions.flatten()\n",
    "        new_longitidunal[list(index_set)] = -1\n",
    "        adata.obs[\"predicted_longitudinal\"] = new_longitidunal\n",
    "        sc.pl.embedding(adata, basis=\"spatial\", color=\"predicted_longitudinal\")\n",
    "\n",
    "        adata.obs[\"not_removed_from_longitudinal\"] = new_longitidunal != -1\n",
    "        adata.obs[\"roll\"] = f\"roll_{roll_names[roll_counter]}\"\n",
    "        adata.write(\n",
    "            os.path.join(\n",
    "                path_adata,\n",
    "                \"adatas\",\n",
    "                f\"05_unrolled_roll_{roll_names[roll_counter]}.h5ad\",\n",
    "            )\n",
    "        )\n",
    "        roll_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(dpi=300)\n",
    "for path_adata in input_folders:\n",
    "    rolls = glob.glob(os.path.join(path_adata, \"adatas\", \"05_unrolled*.h5ad\"))\n",
    "    for roll in rolls:\n",
    "        adata = sc.read(roll)\n",
    "        fig = sc.pl.embedding(\n",
    "            adata[adata.obs[\"not_removed_from_longitudinal\"], :],\n",
    "            basis=\"spatial\",\n",
    "            color=\"predicted_longitudinal\",\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "        try:\n",
    "            os.mkdir(os.path.join(path_adata, \"figures\", \"axes\"))\n",
    "        except:\n",
    "            print(\"axes directory already exists\")\n",
    "        fig.tight_layout()\n",
    "        plt.axis(\"equal\")\n",
    "        fig.savefig(\n",
    "            os.path.join(\n",
    "                path_adata,\n",
    "                \"figures\",\n",
    "                \"axes\",\n",
    "                f\"spatial_longitudinal_{adata.obs.roll.values[0]}.png\",\n",
    "            )\n",
    "        )\n",
    "        fig.savefig(\n",
    "            os.path.join(\n",
    "                r\"C:\\Users\\amonell\\Downloads\\merscope_longitudinal\",\n",
    "                f\"spatial_longitudinal_\"\n",
    "                + os.path.basename(path_adata)\n",
    "                + f\"_{adata.obs.roll.values[0]}.png\",\n",
    "            )\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
