{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "import cv2\n",
    "import pathlib\n",
    "from pyometiff import OMETIFFReader\n",
    "import imageio as io\n",
    "from __future__ import print_function\n",
    "import scanpy as sc\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from tqdm.notebook import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This last script will use the GAN predictions to align the HE with the Xenium DAPI. In summary, we converted the HE image to what the corresponding DAPI image likely looks like. And we are warping this \"likely\" DAPI image to fit the Xenium DAPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the path to the HE and Xenium output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_HE_ome = (\n",
    "    \"/projects/2023_Spatial_Paper/data/HE_timecourse/OMETIFF/d6_SI_r2.ome.tif\"\n",
    ")\n",
    "xenium_path = \"/mnt/sata1/Analysis_Alex/timecourse_replicates/day6_SI_r2/xenium_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_HE_orig = io.imread_v2(path_to_HE_ome)\n",
    "h_and_e = img_array_HE_orig.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the predicted images and stitch them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_path = \"/home/amonell/piloting/pytorch-CycleGAN-and-pix2pix/results/histology_pix2pix/test_latest/images\"\n",
    "\n",
    "predictions = glob.glob(os.path.join(conversion_path, \"*fake*\"))\n",
    "\n",
    "file_paths = predictions\n",
    "\n",
    "sorted_file_paths = sorted(file_paths, key=lambda x: int(x.split(\"_\")[-3]))\n",
    "\n",
    "all_rows = []\n",
    "counter = 0\n",
    "for row in tqdm(range(256, np.shape(h_and_e)[0] + 256, 256)):\n",
    "    current_row = []\n",
    "    for col in range(256, np.shape(h_and_e)[1] + 256, 256):\n",
    "        current_row.append(io.imread(sorted_file_paths[counter]))\n",
    "        counter += 1\n",
    "    all_rows.append(np.hstack(current_row))\n",
    "\n",
    "all_rows = np.vstack(all_rows)\n",
    "img_array_IF_orig = all_rows[:, :, 0][\n",
    "    : np.shape(img_array_HE_orig)[0], : np.shape(img_array_HE_orig)[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsize the predicted DAPI and Xenium DAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_display = 0\n",
    "down_factor = 10\n",
    "\n",
    "new_width = int(img_array_IF_orig.shape[0] / down_factor)\n",
    "new_height = int(img_array_IF_orig.shape[1] / down_factor)\n",
    "# Normalize the image to the range [0, 1]\n",
    "normalized_image = img_array_IF_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image\n",
    "thumbnail = cv2.resize(normalized_image, (new_width, new_height))\n",
    "plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_IF = normalized_image\n",
    "downscale_factor_IF = 3\n",
    "\n",
    "pad_height = (\n",
    "    downscale_factor_IF - img_array_IF.shape[0] % downscale_factor_IF\n",
    ") % downscale_factor_IF\n",
    "pad_width = (\n",
    "    downscale_factor_IF - img_array_IF.shape[1] % downscale_factor_IF\n",
    ") % downscale_factor_IF\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_IF = np.pad(img_array_IF, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "new_width = int(padded_IF.shape[1] / downscale_factor_IF)\n",
    "new_height = int(padded_IF.shape[0] / downscale_factor_IF)\n",
    "\n",
    "# Resize the image\n",
    "resized_image = cv2.resize(padded_IF, (new_width, new_height))\n",
    "IF_image = resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = pathlib.Path(os.path.join(xenium_path, \"morphology_mip.ome.tif\"))\n",
    "\n",
    "reader = OMETIFFReader(fpath=img_fpath)\n",
    "\n",
    "img_array_xenium, metadata_xenium, xml_metadata = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_xenium = 255 * (img_array_xenium / np.max(img_array_xenium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_factor = 10\n",
    "\n",
    "new_width = int(img_array_xenium.shape[1] / down_factor)\n",
    "new_height = int(img_array_xenium.shape[0] / down_factor)\n",
    "\n",
    "# Resize the image\n",
    "thumbnail = cv2.resize(img_array_xenium, (new_width, new_height))\n",
    "plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 3\n",
    "# Calculate the amount of padding needed for each axis\n",
    "pad_height = (\n",
    "    downscale_factor - img_array_xenium.shape[0] % downscale_factor\n",
    ") % downscale_factor\n",
    "pad_width = (\n",
    "    downscale_factor - img_array_xenium.shape[1] % downscale_factor\n",
    ") % downscale_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_array = np.pad(\n",
    "    img_array_xenium, ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    ")\n",
    "\n",
    "# Now 'padded_array' will have both axes sizes divisible by 2\n",
    "img_array_xenium = padded_array\n",
    "\n",
    "new_width = int(img_array_xenium.shape[1] / downscale_factor)\n",
    "new_height = int(img_array_xenium.shape[0] / downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_xenium = cv2.resize(img_array_xenium, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align the predicted DAPI with the actual DAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_constant = 256 / downscale_factor_IF\n",
    "# max number of corners to detect in each image\n",
    "MAX_FEATURES = 10000\n",
    "# percent of corner patches to keep\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    "# how much to blur the initial images to capture villi structure keypoints\n",
    "blur_res_IF = (1, 1)\n",
    "blur_res_xen = (1, 1)\n",
    "\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "\n",
    "    print(\"keypoint detection...\")\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES, patchSize=100)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "    keypoints1_ = [point.pt for point in keypoints1]\n",
    "    keypoints1_sizes = [point.size for point in keypoints1]\n",
    "    keypoints1_angles = [point.angle for point in keypoints1]\n",
    "    # filtered_keypoints = np.array([(p[0], p[1]) for p in keypoints1_ if ((((p[0]%over_constant) > 0) & (p[1]%over_constant > 0)) & (abs(p[0]/over_constant - round(p[0]/over_constant)) > 0.15) & (abs(p[1]/over_constant - round(p[1]/over_constant)) > 0.15))])\n",
    "    ids = np.array(\n",
    "        [\n",
    "            i\n",
    "            for i, p in enumerate(keypoints1_)\n",
    "            if (\n",
    "                (abs(p[0] / over_constant - round(p[0] / over_constant)) > 0.15)\n",
    "                & (abs(p[1] / over_constant - round(p[1] / over_constant)) > 0.15)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    positions_array = np.array([point.pt for point in keypoints1])[ids]\n",
    "    sizes_array = np.array(keypoints1_sizes)[ids]\n",
    "    angles_array = np.array(keypoints1_angles)[ids]\n",
    "\n",
    "    descriptors1 = np.array(descriptors1)[ids]\n",
    "    keypoints1 = []\n",
    "    for i in range(len(positions_array)):\n",
    "        keypoints1.append(\n",
    "            cv2.KeyPoint(\n",
    "                positions_array[i][0],\n",
    "                positions_array[i][1],\n",
    "                size=sizes_array[i],\n",
    "                angle=angles_array[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    "    print(len(keypoints1))\n",
    "    print(len(keypoints2))\n",
    "    print(\"feature matching...\")\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    matches = list(matches)\n",
    "    print(len(matches))\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    print(\"prune bad matches...\")\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "    print(len(matches))\n",
    "    # Draw top matches\n",
    "    # imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    # plt.figure(dpi=200)\n",
    "    # plt.imshow(imMatches)\n",
    "    # plt.show()\n",
    "    # cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Use homography\n",
    "    height, width = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h, keypoints1, keypoints2\n",
    "\n",
    "\n",
    "print(\"Aligning images ...\")\n",
    "# Registered image will be resotred in imReg.\n",
    "# The estimated homography will be stored in h.\n",
    "imReg, h, keypoints1, keypoints2 = alignImages(\n",
    "    cv2.blur(\n",
    "        ((resized_image / np.max(resized_image)) * 255).astype(np.uint8), blur_res_IF\n",
    "    ),\n",
    "    cv2.blur(\n",
    "        ((resized_xenium / np.max(resized_xenium)) * 255).astype(np.uint8), blur_res_xen\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Print estimated homography\n",
    "print(\"Estimated homography : \\n\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = resized_xenium.shape\n",
    "im1Reg = cv2.warpPerspective(resized_image, h, (width, height))\n",
    "\n",
    "new_width = int(im1Reg.shape[1] * downscale_factor_IF)\n",
    "new_height = int(im1Reg.shape[0] * downscale_factor_IF)\n",
    "\n",
    "# Resize the image\n",
    "warped = cv2.resize(im1Reg, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "ax1.imshow(warped[10000:10500, 10000:10500])\n",
    "ax2.imshow(img_array_xenium[10000:10500, 10000:10500])\n",
    "plt.show()\n",
    "# downsize for diplay before this\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "# ax1.imshow(warped)\n",
    "# ax2.imshow(img_array_xenium)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the keypoints from both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keys = []\n",
    "for t in range(len(keypoints1)):\n",
    "    x = keypoints1[t].pt[0]\n",
    "    y = keypoints1[t].pt[1]\n",
    "    plot_keys.append([x, y])\n",
    "plot_keys = np.array(plot_keys)\n",
    "\n",
    "plot_keys2 = []\n",
    "for t in range(len(keypoints2)):\n",
    "    x = keypoints2[t].pt[0]\n",
    "    y = keypoints2[t].pt[1]\n",
    "    plot_keys2.append([x, y])\n",
    "plot_keys2 = np.array(plot_keys2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    cv2.blur(\n",
    "        ((resized_image / np.max(resized_image)) * 255).astype(np.uint8), blur_res_IF\n",
    "    )\n",
    ")\n",
    "plt.scatter(plot_keys[:, 0], plot_keys[:, 1], s=1, c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.blur(resized_xenium.astype(np.uint8), blur_res_xen))\n",
    "plt.scatter(plot_keys2[:, 0], plot_keys2[:, 1], s=1, c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning high quality H&E with low quality aligned H&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array_HE2 = normalized_image\n",
    "downscale_factor = 1\n",
    "\n",
    "pad_height = (\n",
    "    downscale_factor - img_array_HE2.shape[0] % downscale_factor\n",
    ") % downscale_factor\n",
    "pad_width = (\n",
    "    downscale_factor - img_array_HE2.shape[1] % downscale_factor\n",
    ") % downscale_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_HE2 = np.pad(img_array_HE2, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "new_width = int(padded_HE2.shape[1] / downscale_factor)\n",
    "new_height = int(padded_HE2.shape[0] / downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_image2 = cv2.resize(padded_HE2, (new_width, new_height))\n",
    "HE_image2 = resized_image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 2\n",
    "# Calculate the amount of padding needed for each axis\n",
    "pad_height = (downscale_factor - warped.shape[0] % downscale_factor) % downscale_factor\n",
    "pad_width = (downscale_factor - warped.shape[1] % downscale_factor) % downscale_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_array = np.pad(warped, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "# Now 'padded_array' will have both axes sizes divisible by 2\n",
    "warped = padded_array\n",
    "\n",
    "new_width = int(warped.shape[1] / downscale_factor)\n",
    "new_height = int(warped.shape[0] / downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_warped = cv2.resize(warped, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of corners to detect in each image\n",
    "MAX_FEATURES = 4000\n",
    "# percent of corner patches to keep\n",
    "GOOD_MATCH_PERCENT = 0.5\n",
    "# how much to blur the initial images to capture villi structure keypoints\n",
    "blur_res = (5, 5)\n",
    "\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "\n",
    "    print(\"keypoint detection...\")\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES, patchSize=100)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    "    print(len(keypoints1))\n",
    "    print(len(keypoints2))\n",
    "    print(\"feature matching...\")\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    matches = list(matches)\n",
    "    print(len(matches))\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    print(\"prune bad matches...\")\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "    print(len(matches))\n",
    "    # Draw top matches\n",
    "    # imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    # plt.figure(dpi=200)\n",
    "    # plt.imshow(imMatches)\n",
    "    # plt.show()\n",
    "    # cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    print(\"warping\")\n",
    "    # Use homography\n",
    "    height, width = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h\n",
    "\n",
    "\n",
    "print(\"Aligning images ...\")\n",
    "# Registered image will be resotred in imReg.\n",
    "# The estimated homography will be stored in h.\n",
    "imReg, h = alignImages(\n",
    "    cv2.blur(\n",
    "        ((resized_image2 / np.max(resized_image2)) * 255).astype(np.uint8), blur_res\n",
    "    )[:32766, :32766],\n",
    "    resized_warped,\n",
    ")\n",
    "\n",
    "# Print estimated homography\n",
    "print(\"Estimated homography : \\n\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = warped.shape\n",
    "im1Reg = cv2.warpPerspective(resized_image2[:32766, :32766], h, (width, height))\n",
    "\n",
    "new_width = int(im1Reg.shape[1] * downscale_factor)\n",
    "new_height = int(im1Reg.shape[0] * downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "warped2 = cv2.resize(im1Reg, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "ax1.imshow(warped2[10000:15000, 10000:15000])\n",
    "ax2.imshow(warped[10000:15000, 10000:15000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the alignment homography matrix to warp actual H&E images to match the Xenium DAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_he = io.imread_v2(path_to_HE_ome)\n",
    "he_stack = []\n",
    "for i in [0, 1, 2]:\n",
    "    try:\n",
    "        height, width = warped.shape\n",
    "        im1Reg = cv2.warpPerspective(orig_he[:, :, i], h, (width, height))\n",
    "\n",
    "        new_width = int(im1Reg.shape[1] * downscale_factor)\n",
    "        new_height = int(im1Reg.shape[0] * downscale_factor)\n",
    "\n",
    "        # Resize the image\n",
    "        warped2 = cv2.resize(im1Reg, (new_width, new_height))\n",
    "        warped_H_and_E = warped2[\n",
    "            : np.shape(img_array_xenium)[0], : np.shape(img_array_xenium)[1]\n",
    "        ]\n",
    "        he_stack.append(warped_H_and_E)\n",
    "    except:\n",
    "        print(f\"Channel {i} is not in this image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_h_and_e = np.dstack(he_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(transformed_h_and_e[9500:11100, 9500:11100])\n",
    "ax2.imshow(img_array_xenium[9500:11100, 9500:11100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(os.path.dirname(xenium_path), \"h_and_e_alignment_gan.npy\"),\n",
    "    transformed_h_and_e,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merscope_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
