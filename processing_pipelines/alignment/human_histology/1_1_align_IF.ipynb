{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "import cv2\n",
    "import pathlib\n",
    "from pyometiff import OMETIFFReader\n",
    "import imageio as io\n",
    "from __future__ import print_function\n",
    "import scanpy as sc\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put in the paths to the IF images as OME Tiffs and the xenium output folders for all the human IF stainings that you want to align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IF_paths = [\n",
    "    \"/mnt/sata1/Analysis_Alex/human_r1/IF_OMETIFF_human/hSI_05_r1_IF.ome.tif\",\n",
    "    \"/mnt/sata1/Analysis_Alex/human_r1/IF_OMETIFF_human/hSI_05_r2_IF.ome.tif\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xen_paths = [\n",
    "    \"/mnt/sata1/Analysis_Alex/human_r1/human_05_r1/xenium_output\",\n",
    "    \"/mnt/sata1/Analysis_Alex/human_r1/human_05_r2/xenium_output\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform the alignment which saves out the IF images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(IF_paths)):\n",
    "    path_to_IF_ome = IF_paths[k]\n",
    "    xenium_path = xen_paths[k]\n",
    "\n",
    "    img_array_IF_orig = np.max(io.imread_v2(path_to_IF_ome), axis=0)\n",
    "\n",
    "    channel_display = 0\n",
    "    down_factor = 10\n",
    "\n",
    "    new_width = int(img_array_IF_orig.shape[1] / down_factor)\n",
    "    new_height = int(img_array_IF_orig.shape[2] / down_factor)\n",
    "    # Normalize the image to the range [0, 1]\n",
    "    normalized_image = img_array_IF_orig[channel_display, :, :] / np.max(\n",
    "        img_array_IF_orig[channel_display, :, :]\n",
    "    )\n",
    "\n",
    "    # Scale the values back to the range [0, 255] and convert to uint8\n",
    "    normalized_image = (normalized_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize the image\n",
    "    thumbnail = cv2.resize(normalized_image, (new_width, new_height))\n",
    "    plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))\n",
    "    plt.title(\"IF DAPI\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    img_array_IF = normalized_image\n",
    "    downscale_factor_IF = 3\n",
    "\n",
    "    pad_height = (\n",
    "        downscale_factor_IF - img_array_IF.shape[0] % downscale_factor_IF\n",
    "    ) % downscale_factor_IF\n",
    "    pad_width = (\n",
    "        downscale_factor_IF - img_array_IF.shape[1] % downscale_factor_IF\n",
    "    ) % downscale_factor_IF\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    padded_IF = np.pad(img_array_IF, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "    new_width = int(padded_IF.shape[1] / downscale_factor_IF)\n",
    "    new_height = int(padded_IF.shape[0] / downscale_factor_IF)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(padded_IF, (new_width, new_height))\n",
    "    IF_image = resized_image\n",
    "\n",
    "    img_fpath = pathlib.Path(os.path.join(xenium_path, \"morphology_mip.ome.tif\"))\n",
    "\n",
    "    reader = OMETIFFReader(fpath=img_fpath)\n",
    "\n",
    "    img_array_xenium, metadata_xenium, xml_metadata = reader.read()\n",
    "\n",
    "    down_factor = 10\n",
    "\n",
    "    new_width = int(img_array_xenium.shape[1] / down_factor)\n",
    "    new_height = int(img_array_xenium.shape[0] / down_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    thumbnail = cv2.resize(img_array_xenium, (new_width, new_height))\n",
    "    plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))\n",
    "    plt.title(\"Xenium DAPI\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    downscale_factor = 3\n",
    "    # Calculate the amount of padding needed for each axis\n",
    "    pad_height = (\n",
    "        downscale_factor - img_array_xenium.shape[0] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "    pad_width = (\n",
    "        downscale_factor - img_array_xenium.shape[1] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    padded_array = np.pad(\n",
    "        img_array_xenium, ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    "    )\n",
    "\n",
    "    # Now 'padded_array' will have both axes sizes divisible by 2\n",
    "    img_array_xenium = padded_array\n",
    "\n",
    "    new_width = int(img_array_xenium.shape[1] / downscale_factor)\n",
    "    new_height = int(img_array_xenium.shape[0] / downscale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_xenium = cv2.resize(img_array_xenium, (new_width, new_height))\n",
    "\n",
    "    # max number of corners to detect in each image\n",
    "    MAX_FEATURES = 4000\n",
    "    # percent of corner patches to keep\n",
    "    GOOD_MATCH_PERCENT = 0.5\n",
    "    # how much to blur the initial images to capture villi structure keypoints\n",
    "    blur_res_IF = (1, 1)\n",
    "    blur_res_xen = (1, 1)\n",
    "\n",
    "    def alignImages(im1, im2):\n",
    "\n",
    "        print(\"keypoint detection...\")\n",
    "        # Detect ORB features and compute descriptors.\n",
    "        orb = cv2.ORB_create(MAX_FEATURES, patchSize=100)\n",
    "        keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "        keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    "        print(len(keypoints1))\n",
    "        print(len(keypoints2))\n",
    "        print(\"feature matching...\")\n",
    "        # Match features.\n",
    "        matcher = cv2.DescriptorMatcher_create(\n",
    "            cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "        )\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        matches = list(matches)\n",
    "        print(len(matches))\n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        print(\"prune bad matches...\")\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "        print(len(matches))\n",
    "        # Draw top matches\n",
    "        # imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "        # plt.figure(dpi=200)\n",
    "        # plt.imshow(imMatches)\n",
    "        # plt.show()\n",
    "        # cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "        # Find homography\n",
    "        h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "        # Use homography\n",
    "        height, width = im2.shape\n",
    "        im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "        return im1Reg, h\n",
    "\n",
    "    print(\"Aligning images ...\")\n",
    "    # Registered image will be resotred in imReg.\n",
    "    # The estimated homography will be stored in h.\n",
    "    imReg, h = alignImages(\n",
    "        cv2.blur(\n",
    "            ((resized_image / np.max(resized_image)) * 255).astype(np.uint8),\n",
    "            blur_res_IF,\n",
    "        ),\n",
    "        cv2.blur(\n",
    "            ((resized_xenium / np.max(resized_xenium)) * 255).astype(np.uint8),\n",
    "            blur_res_xen,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Print estimated homography\n",
    "    print(\"Estimated homography : \\n\", h)\n",
    "\n",
    "    height, width = resized_xenium.shape\n",
    "    im1Reg = cv2.warpPerspective(resized_image, h, (width, height))\n",
    "\n",
    "    new_width = int(im1Reg.shape[1] * downscale_factor_IF)\n",
    "    new_height = int(im1Reg.shape[0] * downscale_factor_IF)\n",
    "\n",
    "    # Resize the image\n",
    "    warped = cv2.resize(im1Reg, (new_width, new_height))\n",
    "\n",
    "    %matplotlib inline\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    ax1.imshow(warped[10000:10500, 10000:10500])\n",
    "    ax2.imshow(img_array_xenium[10000:10500, 10000:10500])\n",
    "    plt.title(\"First warping iteration\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Starting final warp\")\n",
    "\n",
    "    img_array_HE2 = normalized_image\n",
    "    downscale_factor = 1\n",
    "\n",
    "    pad_height = (\n",
    "        downscale_factor - img_array_HE2.shape[0] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "    pad_width = (\n",
    "        downscale_factor - img_array_HE2.shape[1] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    padded_HE2 = np.pad(\n",
    "        img_array_HE2, ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    "    )\n",
    "\n",
    "    new_width = int(padded_HE2.shape[1] / downscale_factor)\n",
    "    new_height = int(padded_HE2.shape[0] / downscale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image2 = cv2.resize(padded_HE2, (new_width, new_height))\n",
    "    HE_image2 = resized_image2\n",
    "\n",
    "    downscale_factor = 2\n",
    "    # Calculate the amount of padding needed for each axis\n",
    "    pad_height = (\n",
    "        downscale_factor - warped.shape[0] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "    pad_width = (\n",
    "        downscale_factor - warped.shape[1] % downscale_factor\n",
    "    ) % downscale_factor\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    padded_array = np.pad(warped, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "    # Now 'padded_array' will have both axes sizes divisible by 2\n",
    "    warped = padded_array\n",
    "\n",
    "    new_width = int(warped.shape[1] / downscale_factor)\n",
    "    new_height = int(warped.shape[0] / downscale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_warped = cv2.resize(warped, (new_width, new_height))\n",
    "\n",
    "    # max number of corners to detect in each image\n",
    "    MAX_FEATURES = 4000\n",
    "    # percent of corner patches to keep\n",
    "    GOOD_MATCH_PERCENT = 0.5\n",
    "    # how much to blur the initial images to capture villi structure keypoints\n",
    "    blur_res = (5, 5)\n",
    "\n",
    "    def alignImages(im1, im2):\n",
    "\n",
    "        print(\"keypoint detection...\")\n",
    "        # Detect ORB features and compute descriptors.\n",
    "        orb = cv2.ORB_create(MAX_FEATURES, patchSize=100)\n",
    "        keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "        keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    "        print(len(keypoints1))\n",
    "        print(len(keypoints2))\n",
    "        print(\"feature matching...\")\n",
    "        # Match features.\n",
    "        matcher = cv2.DescriptorMatcher_create(\n",
    "            cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "        )\n",
    "        matches = matcher.match(descriptors1, descriptors2, None)\n",
    "        matches = list(matches)\n",
    "        print(len(matches))\n",
    "        # Sort matches by score\n",
    "        matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "        print(\"prune bad matches...\")\n",
    "        # Remove not so good matches\n",
    "        numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "        matches = matches[:numGoodMatches]\n",
    "        print(len(matches))\n",
    "        # Draw top matches\n",
    "        # imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "        # plt.figure(dpi=200)\n",
    "        # plt.imshow(imMatches)\n",
    "        # plt.show()\n",
    "        # cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "        # Extract location of good matches\n",
    "        points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "        points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "            points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "        # Find homography\n",
    "        h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "        print(\"warping\")\n",
    "        # Use homography\n",
    "        height, width = im2.shape\n",
    "        im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "        return im1Reg, h\n",
    "\n",
    "    print(\"Aligning images ...\")\n",
    "    # Registered image will be resotred in imReg.\n",
    "    # The estimated homography will be stored in h.\n",
    "    imReg, h = alignImages(\n",
    "        cv2.blur(\n",
    "            ((resized_image2 / np.max(resized_image2)) * 255).astype(np.uint8), blur_res\n",
    "        )[:32766, :32766],\n",
    "        resized_warped,\n",
    "    )\n",
    "\n",
    "    height, width = warped.shape\n",
    "    im1Reg = cv2.warpPerspective(resized_image2[:32766, :32766], h, (width, height))\n",
    "\n",
    "    new_width = int(im1Reg.shape[1] * downscale_factor)\n",
    "    new_height = int(im1Reg.shape[0] * downscale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    warped2 = cv2.resize(im1Reg, (new_width, new_height))\n",
    "\n",
    "    %matplotlib inline\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    ax1.imshow(warped2[10000:15000, 10000:15000])\n",
    "    ax2.imshow(warped[10000:15000, 10000:15000])\n",
    "    plt.show()\n",
    "\n",
    "    he_stack = []\n",
    "    for i in [0, 1, 2]:\n",
    "        try:\n",
    "            height, width = warped.shape\n",
    "            im1Reg = cv2.warpPerspective(\n",
    "                img_array_IF_orig[i, :, :][:32766, :32766], h, (width, height)\n",
    "            )\n",
    "\n",
    "            new_width = int(im1Reg.shape[1] * downscale_factor)\n",
    "            new_height = int(im1Reg.shape[0] * downscale_factor)\n",
    "\n",
    "            # Resize the image\n",
    "            warped2 = cv2.resize(im1Reg, (new_width, new_height))\n",
    "            warped_H_and_E = warped2[\n",
    "                : np.shape(img_array_xenium)[0], : np.shape(img_array_xenium)[1]\n",
    "            ]\n",
    "            he_stack.append(warped_H_and_E)\n",
    "        except:\n",
    "            print(f\"Channel {i} is not in this image\")\n",
    "\n",
    "    transformed_h_and_e = np.dstack(he_stack)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(transformed_h_and_e[9500:10000, 9500:10000, 0])\n",
    "    ax2.imshow(img_array_xenium[9500:10000, 9500:10000])\n",
    "    plt.show()\n",
    "\n",
    "    np.save(\n",
    "        os.path.join(os.path.dirname(xenium_path), \"IF_alignment.npy\"),\n",
    "        transformed_h_and_e,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merscope_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
