{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning for Cell Annotation Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/amonell/Geneformer\")\n",
    "GPU_NUMBER = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from geneformer import DataCollatorForCellClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Prepare training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset (includes all tissues)\n",
    "train_dataset = load_from_disk(\n",
    "    r\"/mnt/sata1/Analysis_Alex/Geneformer/loom_xenium/tokenized/train_xenium.dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just want to evaluate on our trainset for now\n",
    "eval_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "evalset_list = []\n",
    "organ_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "\n",
    "for organ in Counter(train_dataset[\"organ_major\"]).keys():\n",
    "    # collect list of tissues for fine-tuning\n",
    "    if organ in [\"bone_marrow\"]:\n",
    "        continue\n",
    "    elif organ == \"immune\":\n",
    "        organ_ids = [\"immune\", \"bone_marrow\"]\n",
    "        organ_list += [\"immune\"]\n",
    "    else:\n",
    "        organ_ids = [organ]\n",
    "        organ_list += [organ]\n",
    "\n",
    "    def if_organ(example, organ_id):\n",
    "        return example[\"organ_major\"] in organ_id\n",
    "\n",
    "    trainset_organ = train_dataset.filter(\n",
    "        function=if_organ, fn_kwargs={\"organ_id\": organ_ids}, num_proc=16\n",
    "    )\n",
    "\n",
    "    # per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "    celltype_counter = Counter(trainset_organ[\"cell_type\"])\n",
    "    total_cells = sum(celltype_counter.values())\n",
    "    cells_to_keep = [k for k, v in celltype_counter.items()]\n",
    "\n",
    "    def if_not_rare_celltype(example, cells_to_keep):\n",
    "        return example[\"cell_type\"] in cells_to_keep\n",
    "\n",
    "    trainset_organ_subset = trainset_organ.filter(\n",
    "        if_not_rare_celltype, fn_kwargs={\"cells_to_keep\": cells_to_keep}, num_proc=16\n",
    "    )\n",
    "\n",
    "    # shuffle datasets and rename columns\n",
    "    trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\n",
    "        \"cell_type\", \"label\"\n",
    "    )\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.remove_columns(\"organ_major\")\n",
    "\n",
    "    # create dictionary of cell types : label ids\n",
    "    target_names = list(Counter(trainset_organ_shuffled[\"label\"]).keys())\n",
    "    target_name_id_dict = dict(zip(target_names, [i for i in range(len(target_names))]))\n",
    "    target_dict_list += [target_name_id_dict]\n",
    "\n",
    "    # change labels to numerical ids\n",
    "    def classes_to_ids(example, target_name_id_dict):\n",
    "        example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "        return example\n",
    "\n",
    "    labeled_trainset = trainset_organ_shuffled.map(\n",
    "        classes_to_ids,\n",
    "        fn_kwargs={\"target_name_id_dict\": target_name_id_dict},\n",
    "        num_proc=16,\n",
    "    )\n",
    "\n",
    "    # create 95/5 train/eval splits\n",
    "    labeled_train_split = labeled_trainset.select(\n",
    "        [i for i in range(0, round(len(labeled_trainset) * 0.95))]\n",
    "    )\n",
    "    labeled_eval_split = labeled_trainset.select(\n",
    "        [i for i in range(round(len(labeled_trainset) * 0.95), len(labeled_trainset))]\n",
    "    )\n",
    "\n",
    "    # filter dataset for cell types in corresponding training set\n",
    "    trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "\n",
    "    def if_trained_label(example, trained_labels):\n",
    "        return example[\"label\"] in trained_labels\n",
    "\n",
    "    labeled_eval_split_subset = labeled_eval_split.filter(\n",
    "        if_trained_label, fn_kwargs={\"trained_labels\": trained_labels}, num_proc=16\n",
    "    )\n",
    "\n",
    "    dataset_list += [labeled_train_split]\n",
    "    evalset_list += [labeled_eval_split_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dict = dict(zip(organ_list, dataset_list))\n",
    "traintargetdict_dict = dict(zip(organ_list, target_dict_list))\n",
    "\n",
    "evalset_dict = dict(zip(organ_list, evalset_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Fine-Tune With Cell Classification Learning Objective and Quantify Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2**11  # 2048\n",
    "\n",
    "# set training parameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 0\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 16\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 15\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "# optimizer\n",
    "optimizer = \"adamw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for organ in organ_list:\n",
    "    print(organ)\n",
    "    organ_trainset = trainset_dict[organ]\n",
    "    organ_evalset = evalset_dict[organ]\n",
    "    organ_label_dict = traintargetdict_dict[organ]\n",
    "\n",
    "    # set logging steps\n",
    "    logging_steps = 1\n",
    "\n",
    "    # reload pretrained model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"/mnt/sata1/Analysis_Alex/Geneformer\",\n",
    "        num_labels=len(organ_label_dict.keys()),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = (\n",
    "        f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "    )\n",
    "    output_dir = f\"/mnt/sata1/Analysis_Alex/Geneformer/{datestamp}_geneformer_CellClassifier_SI2_{organ}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}/\"\n",
    "\n",
    "    # ensure not overwriting previously saved model\n",
    "    saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "    if os.path.isfile(saved_model_test) == True:\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "    # make output directory\n",
    "    subprocess.call(f\"mkdir {output_dir}\", shell=True)\n",
    "\n",
    "    # set training arguments\n",
    "    training_args = {\n",
    "        \"learning_rate\": max_lr,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"group_by_length\": True,\n",
    "        \"length_column_name\": \"length\",\n",
    "        \"disable_tqdm\": False,\n",
    "        \"lr_scheduler_type\": lr_schedule_fn,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "        \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"output_dir\": output_dir,\n",
    "    }\n",
    "\n",
    "    training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "    # create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args_init,\n",
    "        data_collator=DataCollatorForCellClassification(),\n",
    "        train_dataset=organ_trainset,\n",
    "        eval_dataset=organ_evalset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # train the cell type classifier\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(organ_evalset)\n",
    "    with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "    trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Predicting with geneformer and saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for filename in glob.glob(\"/mnt/sata1/Analysis_Alex/timecourse_replicates/day*\"):\n",
    "    outname = os.path.basename(filename)\n",
    "    # load train dataset (includes all tissues)\n",
    "    train_dataset = (\n",
    "        \"/mnt/sata1/Analysis_Alex/Geneformer/loom_\"\n",
    "        + outname\n",
    "        + \"/tokenized/train_\"\n",
    "        + outname\n",
    "        + \".dataset\"\n",
    "    )\n",
    "    # load evaluation dataset (includes all tissues)\n",
    "    c = 0\n",
    "    d = train_dataset\n",
    "    # load test\n",
    "    test_dataset = load_from_disk(d)\n",
    "    test_dataset = test_dataset.add_column(\n",
    "        \"label\", [0 for i in range(test_dataset.num_rows)]\n",
    "    )\n",
    "    predictions_test = trainer.predict(test_dataset)\n",
    "\n",
    "    ad = sc.read(\n",
    "        os.path.join(\n",
    "            \"/mnt/sata1/Analysis_Alex/timecourse_replicates\",\n",
    "            outname,\n",
    "            \"adatas/06_reference_mapped.h5ad\",\n",
    "        )\n",
    "    )\n",
    "    ad.obs[\"celltype_predicted\"] = np.array(target_names)[\n",
    "        np.argmax(predictions_test.predictions, axis=1)\n",
    "    ]\n",
    "    ad.write(\n",
    "        os.path.join(\n",
    "            \"/mnt/sata1/Analysis_Alex/timecourse_replicates\",\n",
    "            outname,\n",
    "            \"adatas/07_geneformer_celltypes.h5ad\",\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
