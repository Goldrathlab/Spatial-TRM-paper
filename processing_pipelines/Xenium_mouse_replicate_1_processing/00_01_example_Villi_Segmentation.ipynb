{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 4902390226"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Put the path to the aligned IF image below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenium_path = \"D:/amonell/RaRi/output-XETG00095__0005184__DMSO__20230715__015401\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundary_channel = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_channel_cell_boundary = io.imread(\n",
    "    os.path.join(xenium_path, f\"IF_warped_channel{cell_boundary_channel}.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(if_channel_cell_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(os.path.join(xenium_path, \"villi_images\"))\n",
    "except:\n",
    "    print(\"Villi images directory already made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Chunk the image, and write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "downsize_factor = 5\n",
    "pad_height = (\n",
    "    downsize_factor - if_channel_cell_boundary.shape[0] % downsize_factor\n",
    ") % downsize_factor\n",
    "pad_width = (\n",
    "    downsize_factor - if_channel_cell_boundary.shape[1] % downsize_factor\n",
    ") % downsize_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_array = np.pad(\n",
    "    if_channel_cell_boundary, ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_width = int(padded_array.shape[1] / downsize_factor)\n",
    "new_height = int(padded_array.shape[0] / downsize_factor)\n",
    "\n",
    "image = cv2.resize(padded_array.astype(np.float32), (new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Training Villi segmentation model - Only do if existing villi segmentation model needs changing.\n",
    "We are storing the villi segmentation model that we trained in the ./models folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Make small chunks for training in cellpose GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imwrite(os.path.join(xenium_path, 'villi_images', f'image.png'), image)\n",
    "# Define the chunk size\n",
    "chunk_size = (1000, 1000)\n",
    "\n",
    "for i in tqdm(range(0, image.shape[1], int(chunk_size[1] / 2))):\n",
    "    for j in range(0, image.shape[0], int(chunk_size[0] / 2)):\n",
    "        # Define the coordinates for cropping\n",
    "        left = i\n",
    "        upper = j\n",
    "        right = i + chunk_size[1]\n",
    "        lower = j + chunk_size[0]\n",
    "\n",
    "        # Crop the image chunk\n",
    "        chunk = image[upper:lower, left:right]\n",
    "\n",
    "        cv2.imwrite(\n",
    "            os.path.join(xenium_path, \"villi_images\", f\"chunk_{i}_{j}.png\"),\n",
    "            ((chunk / np.max(chunk)) * 255).astype(int),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Make Large Chunks for faster training once base model is optimized for a few (1000, 1000) chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_remove = glob.glob(os.path.join(xenium_path, \"villi_images\", \"*.png\"))\n",
    "\n",
    "for file_path in files_to_remove:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while removing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = (5000, 5000)\n",
    "\n",
    "for i in tqdm(range(0, image.shape[1], int(chunk_size[1] / 2))):\n",
    "    for j in range(0, image.shape[0], int(chunk_size[0] / 2)):\n",
    "        # Define the coordinates for cropping\n",
    "        left = i\n",
    "        upper = j\n",
    "        right = i + chunk_size[1]\n",
    "        lower = j + chunk_size[0]\n",
    "\n",
    "        # Crop the image chunk\n",
    "        chunk = image[upper:lower, left:right]\n",
    "\n",
    "        cv2.imwrite(\n",
    "            os.path.join(xenium_path, \"villi_images\", f\"chunk_{i}_{j}.png\"),\n",
    "            ((chunk / np.max(chunk)) * 255).astype(int),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Segmenting Villi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We are storing the villi segmentation model that we trained in the ./models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cellpose_model_path = r\"D:/amonell/RaRi/output-XETG00095__0005184__DMSO__20230715__015401/villi_images/models/villi_segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "from cellpose import io as cio\n",
    "\n",
    "model = models.CellposeModel(gpu=True, pretrained_model=cellpose_model_path)\n",
    "channels = [0, 0]\n",
    "masks_, flows_, styles_ = model.eval(\n",
    "    [((image / np.max(image)) * 255).astype(int)],\n",
    "    channels=channels,\n",
    "    diameter=181.35,\n",
    "    flow_threshold=0,\n",
    "    cellprob_threshold=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(xenium_path, \"villi_segmentation_mask.npy\"), masks_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(xenium_path, \"villi_segmentation_downsize_factor.npy\"),\n",
    "    np.array([downsize_factor]),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
