{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#### This is an exmple notebook showing how we align the coordinates between Xenium and IF scan. We ran this notebook separately for every experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "import cv2\n",
    "import pathlib\n",
    "from pyometiff import OMETIFFReader\n",
    "import imageio as io\n",
    "from __future__ import print_function\n",
    "import scanpy as sc\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Go to Qupath > file > open > click on the vsi file of the IF > Fluorescence image > File > Export Images > original pixels > type: ome tiff > specify file name and directory > Let it save out > Enter path to OME TIFF below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_IF_ome = \"D:/amonell/RaRi/IF_scans/Image_Xenium_RARI_IF.ome.ome.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = pathlib.Path(path_to_IF_ome)\n",
    "\n",
    "reader = OMETIFFReader(fpath=img_fpath)\n",
    "\n",
    "img_array_if, metadata_if, xml_metadata = reader.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Display downsized image in the DAPI channel to make sure you're looking at the right experiment (if the dapi image is the first one in the stack then it would be channel 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_display = 0\n",
    "down_factor = 10\n",
    "\n",
    "new_width = int(img_array_if[channel_display].shape[1] / down_factor)\n",
    "new_height = int(img_array_if[channel_display].shape[0] / down_factor)\n",
    "\n",
    "# Resize the image\n",
    "thumbnail = cv2.resize(img_array_if[channel_display], (new_width, new_height))\n",
    "plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## PLEASE MAKE SURE YOU SELECTED THE DAPI CHANNEL FROM IF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We first downsize the image dimensions by 2x, because cv2 has a limit on image size when warping with a matrix transformation. If a stange error is thrown during matrix warping, the images may be to big and will need to be downsized further (This will require changing the padding part in the code below to make our images divisible by 3x, 4x or further instead of 2x). We also need to pad the image beforehand to be divisible by 2, to keep pixel level precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 2\n",
    "\n",
    "pad_height = (\n",
    "    downscale_factor - img_array_if[channel_display].shape[0] % downscale_factor\n",
    ") % downscale_factor\n",
    "pad_width = (\n",
    "    downscale_factor - img_array_if[channel_display].shape[1] % downscale_factor\n",
    ") % downscale_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_IF = np.pad(\n",
    "    img_array_if[channel_display], ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    ")\n",
    "\n",
    "new_width = int(padded_IF.shape[1] / downscale_factor)\n",
    "new_height = int(padded_IF.shape[0] / downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_image = cv2.resize(padded_IF, (new_width, new_height))\n",
    "if_image = resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Read in Xenium morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Put the path to the corresponding Xenium experiment below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenium_path = \"D:/amonell/RaRi/output-XETG00095__0005184__DMSO__20230715__015401\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = pathlib.Path(os.path.join(xenium_path, \"morphology_focus.ome.tif\"))\n",
    "\n",
    "reader = OMETIFFReader(fpath=img_fpath)\n",
    "\n",
    "img_array_xenium, metadata_xenium, xml_metadata = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "down_factor = 10\n",
    "\n",
    "new_width = int(img_array_xenium.shape[1] / down_factor)\n",
    "new_height = int(img_array_xenium.shape[0] / down_factor)\n",
    "\n",
    "# Resize the image\n",
    "thumbnail = cv2.resize(img_array_xenium, (new_width, new_height))\n",
    "plt.imshow(thumbnail, vmax=np.percentile(thumbnail, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Do the same exact padding and downsizing as with the IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_factor = 2\n",
    "# Calculate the amount of padding needed for each axis\n",
    "pad_height = (\n",
    "    downscale_factor - img_array_xenium.shape[0] % downscale_factor\n",
    ") % downscale_factor\n",
    "pad_width = (\n",
    "    downscale_factor - img_array_xenium.shape[1] % downscale_factor\n",
    ") % downscale_factor\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_array = np.pad(\n",
    "    img_array_xenium, ((0, pad_height), (0, pad_width)), mode=\"constant\"\n",
    ")\n",
    "\n",
    "# Now 'padded_array' will have both axes sizes divisible by 2\n",
    "img_array_xenium = padded_array\n",
    "\n",
    "new_width = int(img_array_xenium.shape[1] / downscale_factor)\n",
    "new_height = int(img_array_xenium.shape[0] / downscale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_xenium = cv2.resize(img_array_xenium, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Matching keypoints and warping\n",
    "This is the most important part. You may have to tune some of these parameters to get enough keypoint matches for the transformation to occur. However these parameters work for the initial small intestine rolls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Also can implement a drawing of the keypoint matches, since cv2 function does not allow you to change line weight, but not necessary for functionality so skipping for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of corners to detect in each image\n",
    "MAX_FEATURES = 4000\n",
    "# percent of corner patches to keep\n",
    "GOOD_MATCH_PERCENT = 0.5\n",
    "# how much to blur the initial images to capture villi structure keypoints\n",
    "blur_res = (10, 10)\n",
    "\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "\n",
    "    print(\"keypoint detection...\")\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES, patchSize=100)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2, None)\n",
    "    print(len(keypoints1))\n",
    "    print(len(keypoints2))\n",
    "    print(\"feature matching...\")\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    matches = list(matches)\n",
    "    print(len(matches))\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    print(\"prune bad matches...\")\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "    # Draw top matches\n",
    "    # imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    # plt.figure(dpi=200)\n",
    "    # plt.imshow(imMatches)\n",
    "    # plt.show()\n",
    "    # cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Use homography\n",
    "    height, width = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h\n",
    "\n",
    "\n",
    "print(\"Aligning images ...\")\n",
    "# Registered image will be resotred in imReg.\n",
    "# The estimated homography will be stored in h.\n",
    "imReg, h = alignImages(\n",
    "    cv2.blur(\n",
    "        ((resized_image / np.max(resized_image)) * 255).astype(np.uint8), blur_res\n",
    "    ),\n",
    "    cv2.blur(\n",
    "        ((resized_xenium / np.max(resized_xenium)) * 255).astype(np.uint8), blur_res\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Print estimated homography\n",
    "print(\"Estimated homography : \\n\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Using homography matrix to calculate IF image warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = resized_xenium.shape\n",
    "im1Reg = cv2.warpPerspective(resized_image, h, (width, height))\n",
    "\n",
    "new_width = int(im1Reg.shape[1] * 2)\n",
    "new_height = int(im1Reg.shape[0] * 2)\n",
    "\n",
    "# Resize the image\n",
    "warped = cv2.resize(im1Reg, (new_width, new_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Below should show the warped IF in the first axis, and xenium in the second. If this looks right, continue. If the warped image looks like a giant smear, change the parameters in the keypoint function. This should be accurate to the pixel level, as keypoints are found at the subpixel level. No need for subsequent dapi registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "ax1.imshow(warped[10000:12000, 10000:12000])\n",
    "ax2.imshow(img_array_xenium[10000:12000, 10000:12000])\n",
    "plt.show()\n",
    "# downsize for diplay before this\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "# ax1.imshow(warped)\n",
    "# ax2.imshow(img_array_xenium)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Recreating the IF tiff ome to be the right size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This part of the code is just warping each frame of the IF tiff based on the matrix calculated before, and then saving them out to the xenium data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(np.shape(img_array_if)[0])):\n",
    "    current_img = img_array_if[i]\n",
    "\n",
    "    pad_height = (2 - current_img.shape[0] % 2) % 2\n",
    "    pad_width = (2 - current_img.shape[1] % 2) % 2\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    padded_IF = np.pad(current_img, ((0, pad_height), (0, pad_width)), mode=\"constant\")\n",
    "\n",
    "    new_width_ = int(padded_IF.shape[1] / downscale_factor)\n",
    "    new_height_ = int(padded_IF.shape[0] / downscale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image_ = cv2.resize(padded_IF, (new_width_, new_height_))\n",
    "\n",
    "    height_, width_ = resized_xenium.shape\n",
    "    im1Reg_ = cv2.warpPerspective(resized_image_, h, (width_, height_))\n",
    "\n",
    "    new_width_ = int(im1Reg_.shape[1] * 2)\n",
    "    new_height_ = int(im1Reg_.shape[0] * 2)\n",
    "\n",
    "    warped_ = cv2.resize(im1Reg_, (new_width_, new_height_))\n",
    "    io.imsave(os.path.join(xenium_path, f\"IF_warped_channel{i}.png\"), warped_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Visualization of integration (Not automated, change parts of this to see the channels and regions that you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Grabbing the all IF channels for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.MAX_IMAGE_PIXELS = 4902390226\n",
    "channel_values = io.imread(os.path.join(xenium_path, f\"IF_warped_channel{0}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_values2 = io.imread(os.path.join(xenium_path, f\"IF_warped_channel{1}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(15, 5))\n",
    "ax1.imshow(channel_values[10000:12000, 10000:12000], cmap=\"Reds\")\n",
    "ax1.set_title(\"IF channel 1\")\n",
    "ax2.imshow(channel_values2[10000:12000, 10000:12000], cmap=\"Greens\")\n",
    "ax2.set_title(\"IF channel 2\")\n",
    "ax3.imshow(img_array_xenium[10000:12000, 10000:12000], cmap=\"Blues\")\n",
    "ax3.set_title(\"Xenium DAPI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
