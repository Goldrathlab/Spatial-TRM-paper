{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Postprocessing the Baysor segmentations\n",
    "##### Baysor has many instances where a cell does not overlap with a nucleus, or a cell contains multiple nuclei. This script seeks to correct that. We are very confident in our nuclei segmentations, and therefore are able to make these adjustments with confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "##### This code uses timecourse_env_01 as the anaconda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import alphashape\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from shapely.ops import transform\n",
    "import imageio as io\n",
    "from core_functions.baysor_postprocessing import *\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "##### Put the path to the folders where the Baysor runs are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:/amonell/timecourse_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##### Create anndatas from processing Baysor Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folders = glob.glob(os.path.join(data_dir, \"day*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### To run without multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "for input_file in tqdm(input_folders):\n",
    "    print(input_file)\n",
    "    try:\n",
    "        os.mkdir(os.path.join(input_file, \"adatas\"))\n",
    "    except:\n",
    "        print(\"Adatas dir already exists\")\n",
    "\n",
    "    print(\"Preparing Transcripts...\", end=\" \")\n",
    "    transcripts, transcripts_cellpose = prepare_transcripts(input_file)\n",
    "    print(\"done\")\n",
    "\n",
    "    print(\"Assigning nuclei to Baysor Cells...\", end=\" \")\n",
    "    result = assign_nuclei_to_cells(transcripts, transcripts_cellpose)\n",
    "    print(\"done\")\n",
    "\n",
    "    print(\"Finding the most common nucleus per cell...\", end=\" \")\n",
    "    transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus = (\n",
    "        find_main_nucleus(transcripts, transcripts_cellpose, result)\n",
    "    )\n",
    "    print(\"done\")\n",
    "\n",
    "    print(\"Splitting cells with multiple nucleus assignments...\", end=\" \")\n",
    "    transcripts_with_gt_and_main_nucleus_filtered = reassign_multiple_nuclei(\n",
    "        transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus\n",
    "    )\n",
    "    print(\"done\")\n",
    "\n",
    "    print(\"Making adata...\", end=\" \")\n",
    "    anndata = make_adata(transcripts_with_gt_and_main_nucleus_filtered)\n",
    "    print(\"done\")\n",
    "\n",
    "    anndata.write(os.path.join(input_file, \"adatas\", \"01_preprocessed.h5ad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### To run with multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_folder(input_file):\n",
    "    print(input_file)\n",
    "    try:\n",
    "        os.mkdir(os.path.join(input_file, \"adatas\"))\n",
    "    except:\n",
    "        print(\"Adatas dir already exists\")\n",
    "\n",
    "    transcripts, transcripts_cellpose = prepare_transcripts(input_file)\n",
    "\n",
    "    result = assign_nuclei_to_cells(transcripts, transcripts_cellpose)\n",
    "\n",
    "    transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus = (\n",
    "        find_main_nucleus(transcripts, transcripts_cellpose, result)\n",
    "    )\n",
    "\n",
    "    transcripts_with_gt_and_main_nucleus_filtered = reassign_multiple_nuclei(\n",
    "        transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus\n",
    "    )\n",
    "\n",
    "    anndata = make_adata(transcripts_with_gt_and_main_nucleus_filtered)\n",
    "\n",
    "    anndata.write(os.path.join(input_file, \"adatas\", \"01_preprocessed.h5ad\"))\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=16\n",
    ") as executor:  # You can adjust max_workers as needed\n",
    "    list(\n",
    "        tqdm(\n",
    "            executor.map(process_input_folder, input_folders), total=len(input_folders)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset_fov = transcripts_with_gt_and_main_nucleus_filtered[\n",
    "    (minx < transcripts_with_gt_and_main_nucleus_filtered.y * (1 / 0.2125))\n",
    "    & (transcripts_with_gt_and_main_nucleus_filtered.y * (1 / 0.2125) < maxx)\n",
    "    & (miny < transcripts_with_gt_and_main_nucleus_filtered.x * (1 / 0.2125))\n",
    "    & (transcripts_with_gt_and_main_nucleus_filtered.x * (1 / 0.2125) < maxy)\n",
    "]\n",
    "\n",
    "\n",
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "\n",
    "shapes = (\n",
    "    transcript_subset_fov[~pd.isnull(transcript_subset_fov.cell)]\n",
    "    .groupby(\"split_cell\")[[\"x\", \"y\"]]\n",
    "    .apply(make_alphashape, alpha=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = gpd.GeoSeries(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_pixel_size(path: str) -> float:\n",
    "    file = open(\n",
    "        os.path.join(\n",
    "            \"D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851\",\n",
    "            \"experiment.xenium\",\n",
    "        )\n",
    "    )\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment[\"pixel_size\"]\n",
    "    return pixel_size\n",
    "\n",
    "\n",
    "pixel_size = get_pixel_size(\"\")\n",
    "\n",
    "\n",
    "def scale_to_image(x, y):\n",
    "    return (x / pixel_size, y / pixel_size)\n",
    "\n",
    "\n",
    "# ax.set_xlim((0, np.max(transcript_subset_fov.x.values)))\n",
    "# ax.set_ylim((np.max(transcript_subset_fov.y.values), 0))\n",
    "\n",
    "colors = sns.color_palette()[3]\n",
    "shapes2 = shapes.apply(lambda x: transform(scale_to_image, x))\n",
    "\n",
    "\n",
    "from shapely.affinity import scale\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "img_cropped = img[minx:maxx, miny:maxy]\n",
    "ax.imshow(img_cropped, vmax=np.percentile(img_cropped, 99.9))\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes = gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor=\"none\", alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7, ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter(\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - miny,\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - minx,\n",
    "    s=1,\n",
    "    linewidths=0.01,\n",
    "    alpha=0.5,\n",
    "    c=\"white\",\n",
    ")\n",
    "plt.savefig(\"C:/Users/amonell/Downloads/newest_seg.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dic = cyto_nuc.merge(keydf, left_index=True, right_on=\"cell_number\", how=\"left\")\n",
    "merge_dic[\"inds\"] = [i for i in range(len(merge_dic.index))]\n",
    "groupby_most_common_nucleus = merge_dic.groupby(\"nucleus\")\n",
    "\n",
    "new_cyto_nuc = []\n",
    "new_cell_by_gene = []\n",
    "names = []\n",
    "sets = {}\n",
    "for group_name, group_data in tqdm(groupby_most_common_nucleus):\n",
    "    indices = group_data.inds.values\n",
    "    names.append(group_data.cell_number.values[0])\n",
    "    for m in group_data.cell_number.values:\n",
    "        sets[m] = group_data.cell_number.values[0]\n",
    "    new_cyto_nuc.append(np.sum(cyto_nuc.iloc[indices].values, axis=0))\n",
    "    new_cell_by_gene.append(np.sum(cell_by_gene.iloc[indices].values, axis=0))\n",
    "\n",
    "new_cell_by_gene = np.array(new_cell_by_gene)\n",
    "new_cyto_nuc = np.array(new_cyto_nuc)\n",
    "new_cell_by_gene = pd.DataFrame(\n",
    "    new_cell_by_gene, columns=cell_by_gene.columns, index=names\n",
    ")\n",
    "new_cyto_nuc = pd.DataFrame(new_cyto_nuc, index=names, columns=cyto_nuc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cell_column = [sets.get(p) for p in transcripts.cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts[\"new_cell\"] = new_cell_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### Splitting multi-nucleus cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "anndata = sc.AnnData(\n",
    "    new_cell_by_gene.values,\n",
    "    var=pd.DataFrame(index=new_cell_by_gene.columns),\n",
    "    obs=new_cyto_nuc,\n",
    ")\n",
    "\n",
    "anndata.layers[\"raw\"] = anndata.X\n",
    "anndata.obs[\"cytoplasmic_transcripts\"] = (\n",
    "    anndata.obs[\"total_transcripts\"] - anndata.obs[\"nuclear_transcripts\"]\n",
    ")\n",
    "anndata.obs[\"nuclear_transcript_percentage\"] = (\n",
    "    anndata.obs[\"nuclear_transcripts\"] / anndata.obs[\"total_transcripts\"]\n",
    ")\n",
    "anndata.var[\"gene\"] = anndata.var.index.values\n",
    "anndata.obs[\"cell\"] = anndata.obs.index.values\n",
    "cell_spatial = transcripts.groupby(\"new_cell\")[[\"x\", \"y\"]].mean()\n",
    "anndata.uns[\"points\"] = transcripts\n",
    "anndata.obs = anndata.obs.merge(\n",
    "    cell_spatial, how=\"left\", left_index=True, right_index=True\n",
    ")\n",
    "anndata.obsm[\"X_spatial\"] = anndata.obs[[\"x\", \"y\"]].values\n",
    "anndata = anndata[\n",
    "    :,\n",
    "    ~(\n",
    "        (anndata.var.index.str.contains(\"BLANK\"))\n",
    "        | (anndata.var.index.str.contains(\"NegControl\"))\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_transcripts = transcripts[transcripts.overlaps_nucleus == 1].merge(\n",
    "    transcripts_cellpose[transcripts_cellpose.overlaps_nucleus == 1],\n",
    "    how=\"outer\",\n",
    "    right_index=True,\n",
    "    left_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset = transcripts[transcripts[\"new_cell\"].isin(anndata.obs.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "minx = 8000\n",
    "miny = 20000\n",
    "maxx = 10000\n",
    "maxy = 28000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset_fov = transcript_subset[\n",
    "    transcript_subset[\"new_cell\"].isin(anndata.obs.index)\n",
    "    & (minx < transcript_subset.y * (1 / 0.2125))\n",
    "    & (transcript_subset.y * (1 / 0.2125) < maxx)\n",
    "    & (miny < transcript_subset.x * (1 / 0.2125))\n",
    "    & (transcript_subset.x * (1 / 0.2125) < maxy)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset_all = transcripts[\n",
    "    (minx < transcripts.y * (1 / 0.2125))\n",
    "    & (transcripts.y * (1 / 0.2125) < maxx)\n",
    "    & (miny < transcripts.x * (1 / 0.2125))\n",
    "    & (transcripts.x * (1 / 0.2125) < maxy)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "\n",
    "shapes = (\n",
    "    transcript_subset_fov[~pd.isnull(transcript_subset_fov.cell)]\n",
    "    .groupby(\"new_cell\")[[\"x\", \"y\"]]\n",
    "    .apply(make_alphashape, alpha=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "\n",
    "shapes_all = (\n",
    "    transcript_subset_all[~pd.isnull(transcript_subset_all.cell)]\n",
    "    .groupby(\"cell\")[[\"x\", \"y\"]]\n",
    "    .apply(make_alphashape, alpha=0.05)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = gpd.GeoSeries(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_all = gpd.GeoSeries(shapes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "\n",
    "\n",
    "def import_image(path: str):\n",
    "    file = os.path.join(path, \"morphology_mip.ome.tif\")\n",
    "    img = io.imread(file)\n",
    "    return img\n",
    "\n",
    "\n",
    "img = import_image(\n",
    "    \"D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.affinity as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_pixel_size(path: str) -> float:\n",
    "    file = open(\n",
    "        os.path.join(\n",
    "            \"D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851\",\n",
    "            \"experiment.xenium\",\n",
    "        )\n",
    "    )\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment[\"pixel_size\"]\n",
    "    return pixel_size\n",
    "\n",
    "\n",
    "pixel_size = get_pixel_size(\"\")\n",
    "\n",
    "\n",
    "def scale_to_image(x, y):\n",
    "    return (x / pixel_size, y / pixel_size)\n",
    "\n",
    "\n",
    "# ax.set_xlim((0, np.max(transcript_subset_fov.x.values)))\n",
    "# ax.set_ylim((np.max(transcript_subset_fov.y.values), 0))\n",
    "\n",
    "colors = sns.color_palette()[3]\n",
    "shapes2 = shapes.apply(lambda x: transform(scale_to_image, x))\n",
    "\n",
    "\n",
    "from shapely.affinity import scale\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "img_cropped = img[minx:maxx, miny:maxy]\n",
    "ax.imshow(img_cropped, vmax=np.percentile(img_cropped, 99.9))\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes = gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor=\"none\", alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7, ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter(\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - miny,\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - minx,\n",
    "    s=1,\n",
    "    linewidths=0.01,\n",
    "    alpha=1,\n",
    "    c=\"white\",\n",
    ")\n",
    "plt.savefig(\"C:/Users/amonell/Downloads/seg_no_tan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "img_cropped = img[minx:maxx, miny:maxy]\n",
    "ax.imshow(img_cropped, vmax=np.percentile(img_cropped, 99.9))\n",
    "new_adjusted_shapes = []\n",
    "for a in adjusted_shapes:\n",
    "    try:\n",
    "        new_adjusted_shapes.append(a)\n",
    "    except:\n",
    "        print(\"Not polygon\")\n",
    "new_adjusted_shapes = gpd.GeoSeries(new_adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "new_adjusted_shapes.plot(facecolor=colors, edgecolor=\"none\", alpha=0.2, ax=ax)\n",
    "new_adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7, ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter(\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - miny,\n",
    "    (\n",
    "        transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values\n",
    "        / pixel_size\n",
    "    )\n",
    "    - minx,\n",
    "    s=1,\n",
    "    linewidths=0.01,\n",
    "    alpha=1,\n",
    "    c=\"white\",\n",
    ")\n",
    "plt.savefig(\"C:/Users/amonell/Downloads/seg_no_tan.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_all2 = shapes_all.apply(lambda x: transform(scale_to_image, x))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "img_cropped = img[minx:maxx, miny:maxy]\n",
    "ax.imshow(img_cropped, vmax=np.percentile(img_cropped, 99.9))\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes_all2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes = gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor=\"none\", alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7, ax=ax)\n",
    "plt.scatter(\n",
    "    (transcript_subset_all.x.values / pixel_size) - miny,\n",
    "    (transcript_subset_all.y.values / pixel_size) - minx,\n",
    "    s=1,\n",
    "    linewidths=0.01,\n",
    "    alpha=1,\n",
    "    c=\"white\",\n",
    ")\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = input_folders[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = pd.read_csv(os.path.join(input_file, \"transcripts_cellpose.csv\"), index_col=0)\n",
    "t = pd.read_csv(os.path.join(input_file, \"transcripts.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = t.dropna(subset=[\"cell\"])\n",
    "cell_by_gene = t_.groupby([\"cell\", \"gene\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_nucleus = t_[t_[\"overlaps_nucleus\"] == 1]\n",
    "cell_by_gene_nucleus = (\n",
    "    transcripts_nucleus.groupby([\"cell\", \"gene\"]).size().unstack(fill_value=0)\n",
    ")\n",
    "cell_by_gene_tpc = np.sum(cell_by_gene, axis=1)\n",
    "cell_by_gene_nucleus_tpc = np.sum(cell_by_gene_nucleus, axis=1)\n",
    "cyto_nuc = pd.DataFrame(cell_by_gene_tpc).merge(\n",
    "    pd.DataFrame(cell_by_gene_nucleus_tpc),\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "cyto_nuc.columns = [\"total_transcripts\", \"nuclear_transcripts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyto_nuc = cyto_nuc.fillna(0).astype(int)\n",
    "tc.index = tc.transcript_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_values = t_.cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_values.fillna(\"Not_assigned-0\", inplace=True)\n",
    "t_[\"cell_number\"] = cell_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = t_[t_.overlaps_nucleus == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_associated = tc.loc[overlap.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap[\"associated_nucleus\"] = nuclei_associated.cell_id.values\n",
    "cell_numbers = overlap.cell_number.values\n",
    "associated_nuclei = overlap.associated_nucleus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the most common value for each unique cell number\n",
    "most_common_values = {}\n",
    "\n",
    "# Iterate through the pairs of cell numbers and associated nuclei\n",
    "for cell_number, nucleus in tqdm(zip(cell_numbers, associated_nuclei)):\n",
    "    if cell_number not in most_common_values:\n",
    "        most_common_values[cell_number] = Counter()\n",
    "\n",
    "    most_common_values[cell_number][nucleus] += 1\n",
    "\n",
    "# Calculate the most common nucleus for each unique cell number\n",
    "result = {\n",
    "    cell_number: counter.most_common(1)[0][0]\n",
    "    for cell_number, counter in most_common_values.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(result.keys())\n",
    "values = list(result.values())\n",
    "index = [i for i in range(len(keys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_per = []\n",
    "for cell_number, counter in most_common_values.items():\n",
    "    nuclei_per.append(len(counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap = t_[t_.overlaps_nucleus == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = len(np.unique(non_overlap.cell.values)) - len(\n",
    "    set(np.unique(non_overlap.cell.values)).intersection(\n",
    "        set(np.unique(overlap.cell.values))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(zeros):\n",
    "    nuclei_per.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.clip(nuclei_per, 0, 10), bins=20)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"Number of Cellpose Nuclei\")\n",
    "plt.ylabel(\"Number of Baysor Cells\")\n",
    "plt.xticks([i for i in range(10)])\n",
    "plt.title(\"D6\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_subset = t.iloc[[i for i in range(1000000)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sub = t_subset[t_subset.overlaps_nucleus == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sub_ = t_sub.dropna(subset=[\"cell\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(t_sub_.cell.values)\n",
    "\n",
    "# import random\n",
    "# color_map = {value: (random.random(), random.random(), random.random()) for value in tqdm(unique_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors1 = [color_map[value] for value in tqdm(t_sub_.cell.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(t_sub.x.values[:3000], t_sub.y.values[:3000], s=1, c=colors[:3000])\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(t_.x.values[:3000], t_.y.values[:3000], s=1, c=t_.overlaps_nucleus.values[:3000])\n",
    "\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_sub = t_subset.merge(tc, left_index=True, right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sub2 = tc_sub[tc_sub.overlaps_nucleus_y == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(ts_sub2.cell_id.values)\n",
    "\n",
    "# import random\n",
    "# color_map = {value: (random.random(), random.random(), random.random()) for value in tqdm(unique_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [color_map[value] for value in tqdm(ts_sub2.cell_id.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "# ax1.scatter(ts_sub2.x_location.values[:3000], ts_sub2.y_location.values[:3000], s=1, c=colors[:3000])\n",
    "# ax2.scatter(t_sub_.x.values[:3000], t_sub_.y.values[:3000], s=1, c=colors1[:3000])\n",
    "# ax1.axis('equal')\n",
    "# ax2.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the most common value for each unique cell number\n",
    "most_common_values = {}\n",
    "\n",
    "# Iterate through the pairs of cell numbers and associated nuclei\n",
    "for cell_number, nucleus in tqdm(zip(ts_sub2.cell.values, ts_sub2.cell_id.values)):\n",
    "    if cell_number not in most_common_values:\n",
    "        most_common_values[cell_number] = Counter()\n",
    "\n",
    "    most_common_values[cell_number][nucleus] += 1\n",
    "\n",
    "# Calculate the most common nucleus for each unique cell number\n",
    "result = {\n",
    "    cell_number: counter.most_common(1)[0][0]\n",
    "    for cell_number, counter in most_common_values.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "# Example clusterings (replace with your own data)\n",
    "predicted = [result.get(i) for i in ts_sub2.cell.values[:20000]]\n",
    "real = ts_sub2.cell_id.values[:20000]\n",
    "\n",
    "ami = adjusted_mutual_info_score(predicted, real)\n",
    "print(\"Adjusted Mutual Information:\", ami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_d6 = sc.read(\n",
    "    r\"D:\\amonell\\timecourse\\output-XETG00095__0011274__SI_d6__20230825__004851\\adatas\\preprocessed_and_filtered_02.h5ad\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = np.unique(tc.cell_id.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = np.unique(transcripts_nucleus.cell.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt[0][np.argmax(rt[1][1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(rt[1] > 20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rt[1][1:], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rc[1][1:], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ol_d6.obs[\"transcript_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.unique(ts_sub2.cell_id, return_counts=True)[1], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = sc.read(os.path.join(input_folders[1], \"adatas\", \"preprocessed_01.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(experiment.obs.nuclear_transcripts.values > 20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(experiment.obs.nuclear_transcripts.values, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = input_folders[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_cellpose = pd.read_csv(\n",
    "    os.path.join(input_file, \"transcripts_cellpose.csv\"), index_col=0\n",
    ")\n",
    "transcripts = pd.read_csv(os.path.join(input_file, \"transcripts.csv\"), index_col=0)\n",
    "\n",
    "transcripts = transcripts.dropna(subset=[\"cell\"])\n",
    "cell_by_gene = transcripts.groupby([\"cell\", \"gene\"]).size().unstack(fill_value=0)\n",
    "transcripts_nucleus = transcripts[transcripts[\"overlaps_nucleus\"] == 1]\n",
    "cell_by_gene_nucleus = (\n",
    "    transcripts_nucleus.groupby([\"cell\", \"gene\"]).size().unstack(fill_value=0)\n",
    ")\n",
    "cell_by_gene_tpc = np.sum(cell_by_gene, axis=1)\n",
    "cell_by_gene_nucleus_tpc = np.sum(cell_by_gene_nucleus, axis=1)\n",
    "cyto_nuc = pd.DataFrame(cell_by_gene_tpc).merge(\n",
    "    pd.DataFrame(cell_by_gene_nucleus_tpc),\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "cyto_nuc.columns = [\"total_transcripts\", \"nuclear_transcripts\"]\n",
    "\n",
    "cyto_nuc = cyto_nuc.fillna(0).astype(int)\n",
    "transcripts_cellpose.index = transcripts_cellpose.transcript_id.values\n",
    "cell_values = transcripts.cell\n",
    "cell_values.fillna(\"Not_assigned-0\", inplace=True)\n",
    "transcripts[\"cell_number\"] = cell_values\n",
    "overlap = transcripts[transcripts.overlaps_nucleus == 1]\n",
    "nuclei_associated = transcripts_cellpose.loc[overlap.index.values]\n",
    "overlap[\"associated_nucleus\"] = nuclei_associated.cell_id.values\n",
    "cell_numbers = overlap.cell_number.values\n",
    "associated_nuclei = overlap.associated_nucleus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Create a dictionary to store the most common value for each unique cell number\n",
    "    most_common_values = {}\n",
    "\n",
    "    # Iterate through the pairs of cell numbers and associated nuclei\n",
    "    for cell_number, nucleus in tqdm(zip(cell_numbers, associated_nuclei)):\n",
    "        if cell_number not in most_common_values:\n",
    "            most_common_values[cell_number] = Counter()\n",
    "    \n",
    "        most_common_values[cell_number][nucleus] += 1\n",
    "    \n",
    "    # Calculate the most common nucleus for each unique cell number\n",
    "    result = {cell_number: counter.most_common(1)[0][0] for cell_number, counter in most_common_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "    keys = list(result.keys())\n",
    "    values = list(result.values())\n",
    "    index = [i for i in range(len(keys))]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "keydf = pd.DataFrame(\n",
    "    zip(keys, values, index), columns=[\"cell_number\", \"nucleus\", \"inds\"]\n",
    ")\n",
    "merge_dic = cyto_nuc.merge(keydf, left_index=True, right_on=\"cell_number\", how=\"left\")\n",
    "merge_dic[\"inds\"] = [i for i in range(len(merge_dic.index))]\n",
    "groupby_most_common_nucleus = merge_dic.groupby(\"nucleus\")\n",
    "\n",
    "new_cyto_nuc = []\n",
    "new_cell_by_gene = []\n",
    "names = []\n",
    "for group_name, group_data in tqdm(groupby_most_common_nucleus):\n",
    "    indices = group_data.inds.values\n",
    "    names.append(group_data.cell_number.values[0])\n",
    "    new_cyto_nuc.append(np.sum(cyto_nuc.iloc[indices].values, axis=0))\n",
    "    new_cell_by_gene.append(np.sum(cell_by_gene.iloc[indices].values, axis=0))\n",
    "\n",
    "new_cell_by_gene = np.array(new_cell_by_gene)\n",
    "new_cyto_nuc = np.array(new_cyto_nuc)\n",
    "new_cell_by_gene = pd.DataFrame(\n",
    "    new_cell_by_gene, columns=cell_by_gene.columns, index=names\n",
    ")\n",
    "new_cyto_nuc = pd.DataFrame(new_cyto_nuc, index=names, columns=cyto_nuc.columns)\n",
    "\n",
    "anndata = sc.AnnData(\n",
    "    new_cell_by_gene.values,\n",
    "    var=pd.DataFrame(index=new_cell_by_gene.columns),\n",
    "    obs=new_cyto_nuc,\n",
    ")\n",
    "\n",
    "anndata.layers[\"raw\"] = anndata.X\n",
    "anndata.obs[\"cytoplasmic_transcripts\"] = (\n",
    "    anndata.obs[\"total_transcripts\"] - anndata.obs[\"nuclear_transcripts\"]\n",
    ")\n",
    "anndata.obs[\"nuclear_transcript_percentage\"] = (\n",
    "    anndata.obs[\"nuclear_transcripts\"] / anndata.obs[\"total_transcripts\"]\n",
    ")\n",
    "anndata.var[\"gene\"] = anndata.var.index.values\n",
    "anndata.obs[\"cell\"] = anndata.obs.index.values\n",
    "cell_spatial = transcripts.groupby(\"cell\")[[\"x\", \"y\"]].mean()\n",
    "anndata.obs = anndata.obs.merge(\n",
    "    cell_spatial, how=\"left\", left_index=True, right_index=True\n",
    ")\n",
    "anndata.obsm[\"X_spatial\"] = anndata.obs[[\"x\", \"y\"]].values\n",
    "anndata = anndata[\n",
    "    :,\n",
    "    ~(\n",
    "        (anndata.var.index.str.contains(\"BLANK\"))\n",
    "        | (anndata.var.index.str.contains(\"NegControl\"))\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "anndata[anndata.obs[\"nuclear_transcripts\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in tqdm(input_folders):\n",
    "    experiment = sc.read(os.path.join(input_file, \"adatas\", \"preprocessed_01.h5ad\"))\n",
    "    try:\n",
    "        df = pd.DataFrame(\n",
    "            experiment.X.A,\n",
    "            columns=experiment.var.index.values,\n",
    "            index=experiment.obs.index.values,\n",
    "        )\n",
    "    except:\n",
    "        df = pd.DataFrame(\n",
    "            experiment.X,\n",
    "            columns=experiment.var.index.values,\n",
    "            index=experiment.obs.index.values,\n",
    "        )\n",
    "\n",
    "    metadata = experiment.obs\n",
    "    print(\"QC metrics for batch \" + os.path.basename(input_file))\n",
    "\n",
    "    plot_qc_feature(df, metadata, False)\n",
    "\n",
    "    default_parameters = input(\"Do you want to use default filtering cutoffs (y/n)?\")\n",
    "\n",
    "    if default_parameters == \"n\":\n",
    "\n",
    "        min_transcript_threshold = float(input(\"Min transcripts threshold: \"))\n",
    "        max_transcript_threshold = float(input(\"Max transcripts threshold: \"))\n",
    "\n",
    "        min_nuclear_transcripts = float(input(\"Min nuclear transcripts: \"))\n",
    "        max_nuclear_transcripts = float(input(\"Max nuclear transcripts: \"))\n",
    "\n",
    "        min_cyto_transcripts = float(input(\"Min cyto transcripts: \"))\n",
    "        max_cyto_transcripts = float(input(\"Max cyto transcripts: \"))\n",
    "\n",
    "        min_nuc_pct = float(input(\"Min nuclear transcripts / total transcripts: \"))\n",
    "        max_nuc_pct = float(input(\"Max nuclear transcripts / total transcripts: \"))\n",
    "\n",
    "        experiment = qc_before_clustering(\n",
    "            experiment,\n",
    "            min_transcript_threshold,\n",
    "            max_transcript_threshold,\n",
    "            min_nuclear_transcripts,\n",
    "            max_nuclear_transcripts,\n",
    "            min_cyto_transcripts,\n",
    "            max_cyto_transcripts,\n",
    "            min_nuc_pct,\n",
    "            max_nuc_pct,\n",
    "        )\n",
    "    else:\n",
    "        experiment = qc_before_clustering(experiment)\n",
    "    experiment.write(\n",
    "        os.path.join(input_file, \"adatas\", \"preprocessed_and_filtered_02.h5ad\")\n",
    "    )\n",
    "    # Add lines to save out figures into new analysis subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in tqdm(input_folders):\n",
    "    experiment = sc.read(\n",
    "        os.path.join(input_file, \"adatas\", \"preprocessed_and_filtered_02.h5ad\")\n",
    "    )\n",
    "    sc.tl.pca(experiment)\n",
    "    sc.pp.neighbors(experiment)\n",
    "    sc.tl.leiden(experiment, key_added=\"original_leiden\")\n",
    "    sc.tl.umap(experiment)\n",
    "    experiment.write(\n",
    "        os.path.join(input_file, \"adatas\", \"initial_umap_calculated_03.h5ad\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
