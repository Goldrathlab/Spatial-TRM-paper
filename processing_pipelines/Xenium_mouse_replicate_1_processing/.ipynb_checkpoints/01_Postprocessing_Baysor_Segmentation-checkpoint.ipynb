{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e85e866",
   "metadata": {},
   "source": [
    "### Postprocessing the Baysor segmentations\n",
    "##### Baysor has many instances where a cell does not overlap with a nucleus, or a cell contains multiple nuclei. This script seeks to correct that. We are very confident in our nuclei segmentations, and therefore are able to make these adjustments with confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f3cd7",
   "metadata": {},
   "source": [
    "##### This code uses timecourse_env_01 as the anaconda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cf849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import alphashape\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from shapely.ops import transform\n",
    "import imageio as io\n",
    "from core_functions.baysor_postprocessing import *\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea17f0",
   "metadata": {},
   "source": [
    "##### Put the path to the folders where the Baysor runs are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a152ea5e-012e-48a6-8386-69dd4b716c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/amonell/timecourse_final'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c858a-7f9a-4069-8311-514b289448b9",
   "metadata": {},
   "source": [
    "##### Create anndatas from processing Baysor Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d04d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folders = glob.glob(os.path.join(data_dir, 'day*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c904678-57b4-4f8e-991c-59fa3923df26",
   "metadata": {},
   "source": [
    "#### To run without multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9db62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "for input_file in tqdm(input_folders):\n",
    "    print(input_file)\n",
    "    try: \n",
    "        os.mkdir(os.path.join(input_file, 'adatas'))\n",
    "    except:\n",
    "        print('Adatas dir already exists')\n",
    "\n",
    "    print('Preparing Transcripts...', end = ' ')\n",
    "    transcripts, transcripts_cellpose = prepare_transcripts(input_file)\n",
    "    print('done')\n",
    "    \n",
    "    print('Assigning nuclei to Baysor Cells...', end = ' ')\n",
    "    result = assign_nuclei_to_cells(transcripts, transcripts_cellpose)\n",
    "    print('done')\n",
    "\n",
    "    print('Finding the most common nucleus per cell...', end = ' ')\n",
    "    transcripts_with_gt_and_main_nucleus_filtered,  groupby_most_common_nucleus = find_main_nucleus(transcripts, transcripts_cellpose, result)\n",
    "    print('done')\n",
    "\n",
    "    print('Splitting cells with multiple nucleus assignments...', end = ' ')\n",
    "    transcripts_with_gt_and_main_nucleus_filtered = reassign_multiple_nuclei(transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus)\n",
    "    print('done')\n",
    "\n",
    "    print('Making adata...', end = ' ')\n",
    "    anndata = make_adata(transcripts_with_gt_and_main_nucleus_filtered)\n",
    "    print('done')\n",
    "    \n",
    "    anndata.write(os.path.join(input_file, 'adatas', '01_preprocessed.h5ad'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2db47-3e7d-4c68-8733-df438d2b4ef2",
   "metadata": {},
   "source": [
    "#### To run with multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a137aa-7975-4d92-bfda-fb48201e2d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_input_folder(input_file):\n",
    "    print(input_file)\n",
    "    try: \n",
    "        os.mkdir(os.path.join(input_file, 'adatas'))\n",
    "    except:\n",
    "        print('Adatas dir already exists')\n",
    "\n",
    "    transcripts, transcripts_cellpose = prepare_transcripts(input_file)\n",
    "    \n",
    "    result = assign_nuclei_to_cells(transcripts, transcripts_cellpose)\n",
    "\n",
    "    transcripts_with_gt_and_main_nucleus_filtered,  groupby_most_common_nucleus = find_main_nucleus(transcripts, transcripts_cellpose, result)\n",
    "\n",
    "    transcripts_with_gt_and_main_nucleus_filtered = reassign_multiple_nuclei(transcripts_with_gt_and_main_nucleus_filtered, groupby_most_common_nucleus)\n",
    "\n",
    "    anndata = make_adata(transcripts_with_gt_and_main_nucleus_filtered)\n",
    "    \n",
    "    anndata.write(os.path.join(input_file, 'adatas', '01_preprocessed.h5ad'))\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:  # You can adjust max_workers as needed\n",
    "    list(tqdm(executor.map(process_input_folder, input_folders), total=len(input_folders)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f09bd-03fb-4fbf-8aba-57112caab456",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset_fov = transcripts_with_gt_and_main_nucleus_filtered[(minx < transcripts_with_gt_and_main_nucleus_filtered.y*(1/.2125)) & (transcripts_with_gt_and_main_nucleus_filtered.y*(1/.2125) < maxx)& (miny < transcripts_with_gt_and_main_nucleus_filtered.x*(1/.2125)) & (transcripts_with_gt_and_main_nucleus_filtered.x*(1/.2125) < maxy)]\n",
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "shapes = transcript_subset_fov[~pd.isnull(transcript_subset_fov.cell)].groupby(\"split_cell\")[['x', 'y']].apply(make_alphashape, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f94147-5c3e-448a-b0fe-22f16a8d9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = gpd.GeoSeries(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc97a28-ac41-4fb6-8c90-099303622ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_pixel_size(path: str) -> float:\n",
    "    file = open(os.path.join('D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851', \"experiment.xenium\"))\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment['pixel_size']\n",
    "    return pixel_size\n",
    "\n",
    "pixel_size = get_pixel_size(\"\")\n",
    "def scale_to_image(x, y):\n",
    "    return(x/pixel_size, y/pixel_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_xlim((0, np.max(transcript_subset_fov.x.values)))\n",
    "#ax.set_ylim((np.max(transcript_subset_fov.y.values), 0))\n",
    "\n",
    "colors = sns.color_palette()[3]\n",
    "shapes2 = shapes.apply(lambda x: transform(scale_to_image, x))\n",
    "\n",
    "\n",
    "from shapely.affinity import scale\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15,15))\n",
    "img_cropped=img[minx:maxx, miny:maxy]\n",
    "ax.imshow(\n",
    "    img_cropped, \n",
    "    vmax=np.percentile(img_cropped, 99.9)\n",
    ")\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes =gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor='none', alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7,  ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter((transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values/pixel_size) - miny, (transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values/pixel_size) - minx, s=1, linewidths=0.01, alpha=0.5, c='white')\n",
    "plt.savefig('C:/Users/amonell/Downloads/newest_seg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6b06f-e42b-4b51-aee4-908490e2da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_dic = cyto_nuc.merge(keydf, left_index=True, right_on='cell_number', how='left')\n",
    "merge_dic['inds'] = [i for i in range(len(merge_dic.index))]\n",
    "groupby_most_common_nucleus = merge_dic.groupby('nucleus')\n",
    "\n",
    "new_cyto_nuc = []\n",
    "new_cell_by_gene = []\n",
    "names = []\n",
    "sets = {}\n",
    "for group_name, group_data in tqdm(groupby_most_common_nucleus):\n",
    "    indices = group_data.inds.values\n",
    "    names.append(group_data.cell_number.values[0])\n",
    "    for m in group_data.cell_number.values:\n",
    "        sets[m] = group_data.cell_number.values[0]\n",
    "    new_cyto_nuc.append(np.sum(cyto_nuc.iloc[indices].values, axis=0))\n",
    "    new_cell_by_gene.append(np.sum(cell_by_gene.iloc[indices].values, axis=0))\n",
    "    \n",
    "new_cell_by_gene = np.array(new_cell_by_gene)\n",
    "new_cyto_nuc = np.array(new_cyto_nuc) \n",
    "new_cell_by_gene = pd.DataFrame(new_cell_by_gene, columns=cell_by_gene.columns, index=names)\n",
    "new_cyto_nuc=pd.DataFrame(new_cyto_nuc, index=names, columns=cyto_nuc.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c7e42-992d-4188-b6ba-802da98cb93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b2942-fd67-436f-b571-56f3143be9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cell_column = [sets.get(p) for p in transcripts.cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435fabe-290a-45d5-bd78-de3ae3bfd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts['new_cell'] = new_cell_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72346c9-b98f-4ef9-9d6f-3a9dd1f3b690",
   "metadata": {},
   "source": [
    "##### Splitting multi-nucleus cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e878d9-320f-4cec-9315-2220cca83823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c3b2b-b67e-481c-8399-c9e49e0331b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543806c4-f0c3-4b84-8065-73d757d18809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "anndata = sc.AnnData(new_cell_by_gene.values, var=pd.DataFrame(index=new_cell_by_gene.columns), obs=new_cyto_nuc)\n",
    "\n",
    "anndata.layers['raw'] = anndata.X\n",
    "anndata.obs['cytoplasmic_transcripts'] = anndata.obs['total_transcripts'] - anndata.obs['nuclear_transcripts'] \n",
    "anndata.obs['nuclear_transcript_percentage'] = anndata.obs['nuclear_transcripts']/anndata.obs['total_transcripts']\n",
    "anndata.var['gene'] = anndata.var.index.values\n",
    "anndata.obs['cell'] = anndata.obs.index.values\n",
    "cell_spatial = transcripts.groupby('new_cell')[['x', 'y']].mean()\n",
    "anndata.uns['points'] = transcripts\n",
    "anndata.obs = anndata.obs.merge(cell_spatial, how='left', left_index=True, right_index=True)\n",
    "anndata.obsm['X_spatial'] = anndata.obs[['x', 'y']].values\n",
    "anndata = anndata[:, ~((anndata.var.index.str.contains('BLANK')) | (anndata.var.index.str.contains('NegControl')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17e55c-9114-472b-b0a4-4898d77b2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_transcripts = transcripts[transcripts.overlaps_nucleus == 1].merge(transcripts_cellpose[transcripts_cellpose.overlaps_nucleus == 1], how='outer', right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffe79f-86a5-48ce-92a4-b82944da62e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726e0c0-da49-421e-bf24-e158412aa126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7ea29-ec09-4611-ad29-3349ba9c5697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript_subset = transcripts[transcripts['new_cell'].isin(anndata.obs.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cdd4a2-f78b-4770-9a79-ff1bfe62e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minx=8000\n",
    "miny=20000\n",
    "maxx=10000\n",
    "maxy=28000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537229a4-0711-4114-9a3b-5fb61dd22bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript_subset_fov = transcript_subset[transcript_subset['new_cell'].isin(anndata.obs.index) & (minx < transcript_subset.y*(1/.2125)) & (transcript_subset.y*(1/.2125) < maxx)& (miny < transcript_subset.x*(1/.2125)) & (transcript_subset.x*(1/.2125) < maxy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b939a03-c061-435a-acd3-7d49a6511373",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_subset_all = transcripts[(minx < transcripts.y*(1/.2125)) & (transcripts.y*(1/.2125) < maxx)& (miny < transcripts.x*(1/.2125)) & (transcripts.x*(1/.2125) < maxy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e3379-c69a-452f-82b7-a154e70fb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "shapes = transcript_subset_fov[~pd.isnull(transcript_subset_fov.cell)].groupby(\"new_cell\")[['x', 'y']].apply(make_alphashape, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f4871-e877-4825-b3e8-a7468a98e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alphashape(points: pd.DataFrame, alpha: float):\n",
    "    points = np.array(points)\n",
    "    shape = alphashape.alphashape(points, alpha=alpha)\n",
    "    return shape\n",
    "\n",
    "shapes_all = transcript_subset_all[~pd.isnull(transcript_subset_all.cell)].groupby(\"cell\")[['x', 'y']].apply(make_alphashape, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efdbc2-e229-4bb6-aa6a-a767fd9aab4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff2d22-cc46-4cfb-89e5-ece12266edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = gpd.GeoSeries(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164da8be-dab9-45b7-9b45-5c50b85a00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shapes_all = gpd.GeoSeries(shapes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d3fe7-7f8b-4ab0-8934-50d90e7acb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "def import_image(path: str):\n",
    "    file = os.path.join(path, \"morphology_mip.ome.tif\")\n",
    "    img = io.imread(file)\n",
    "    return img\n",
    "img = import_image('D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecbe07-7714-4131-9ef1-a94a98f11e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely.affinity as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b3dee-6d69-4fc8-a89a-627efe8f4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_pixel_size(path: str) -> float:\n",
    "    file = open(os.path.join('D:/amonell/timecourse/output-XETG00095__0011274__SI_d6__20230825__004851', \"experiment.xenium\"))\n",
    "    experiment = json.load(file)\n",
    "    pixel_size = experiment['pixel_size']\n",
    "    return pixel_size\n",
    "\n",
    "pixel_size = get_pixel_size(\"\")\n",
    "def scale_to_image(x, y):\n",
    "    return(x/pixel_size, y/pixel_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ax.set_xlim((0, np.max(transcript_subset_fov.x.values)))\n",
    "#ax.set_ylim((np.max(transcript_subset_fov.y.values), 0))\n",
    "\n",
    "colors = sns.color_palette()[3]\n",
    "shapes2 = shapes.apply(lambda x: transform(scale_to_image, x))\n",
    "\n",
    "\n",
    "from shapely.affinity import scale\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15,15))\n",
    "img_cropped=img[minx:maxx, miny:maxy]\n",
    "ax.imshow(\n",
    "    img_cropped, \n",
    "    vmax=np.percentile(img_cropped, 99.9)\n",
    ")\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes =gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor='none', alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7,  ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter((transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values/pixel_size) - miny, (transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values/pixel_size) - minx, s=1, linewidths=0.01, alpha=1, c='white')\n",
    "plt.savefig('C:/Users/amonell/Downloads/seg_no_tan.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5be4fb-5003-4ecd-9214-76ba6bcc4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15,15))\n",
    "img_cropped=img[minx:maxx, miny:maxy]\n",
    "ax.imshow(\n",
    "    img_cropped, \n",
    "    vmax=np.percentile(img_cropped, 99.9)\n",
    ")\n",
    "new_adjusted_shapes = []\n",
    "for a in adjusted_shapes:\n",
    "    try:\n",
    "        new_adjusted_shapes.append(a)\n",
    "    except:\n",
    "        print('Not polygon')\n",
    "new_adjusted_shapes =gpd.GeoSeries(new_adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "new_adjusted_shapes.plot(facecolor=colors, edgecolor='none', alpha=0.2, ax=ax)\n",
    "new_adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7,  ax=ax)\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.scatter((transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].x.values/pixel_size) - miny, (transcript_subset_all[transcript_subset_all.overlaps_nucleus == 1].y.values/pixel_size) - minx, s=1, linewidths=0.01, alpha=1, c='white')\n",
    "plt.savefig('C:/Users/amonell/Downloads/seg_no_tan.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc9f36-7a8f-4a02-99e0-93171a1ae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_all2 = shapes_all.apply(lambda x: transform(scale_to_image, x))\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15,15))\n",
    "img_cropped=img[minx:maxx, miny:maxy]\n",
    "ax.imshow(\n",
    "    img_cropped, \n",
    "    vmax=np.percentile(img_cropped, 99.9)\n",
    ")\n",
    "# Create an empty GeoDataFrame to store adjusted polygons\n",
    "adjusted_shapes = []\n",
    "\n",
    "# Iterate through the shapes DataFrame and adjust each polygon\n",
    "for original_polygon in shapes_all2:\n",
    "    scaled_polygon = sa.translate(original_polygon, -miny, -minx)\n",
    "    adjusted_shapes.append(scaled_polygon)\n",
    "adjusted_shapes =gpd.GeoSeries(adjusted_shapes)\n",
    "# Plot the adjusted polygons\n",
    "adjusted_shapes.plot(facecolor=colors, edgecolor='none', alpha=0.2, ax=ax)\n",
    "adjusted_shapes.plot(facecolor=\"none\", edgecolor=colors, alpha=0.7,  ax=ax)\n",
    "plt.scatter((transcript_subset_all.x.values/pixel_size) - miny, (transcript_subset_all.y.values/pixel_size) - minx, s=1, linewidths=0.01, alpha=1, c='white')\n",
    "ax.set_xlim((0, 1000))\n",
    "# ax.set_ylim((1500, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486de14a-2ea0-4d5e-a152-5b815aa49e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = input_folders[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c56d3-fc62-404d-bf72-99e2ad24fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc= pd.read_csv(os.path.join(input_file, 'transcripts_cellpose.csv'), index_col=0)\n",
    "t= pd.read_csv(os.path.join(input_file, 'transcripts.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e1b08-00da-4319-a19a-735bd3160a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = t.dropna(subset=['cell'])\n",
    "cell_by_gene = t_.groupby(['cell', 'gene']).size().unstack(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec93785-3873-4579-b90f-3349d6a318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_nucleus = t_[t_['overlaps_nucleus'] == 1]\n",
    "cell_by_gene_nucleus = transcripts_nucleus.groupby(['cell', 'gene']).size().unstack(fill_value=0)\n",
    "cell_by_gene_tpc = np.sum(cell_by_gene, axis=1)\n",
    "cell_by_gene_nucleus_tpc = np.sum(cell_by_gene_nucleus, axis=1)\n",
    "cyto_nuc = pd.DataFrame(cell_by_gene_tpc).merge(pd.DataFrame(cell_by_gene_nucleus_tpc), how='outer', left_index=True, right_index=True)\n",
    "cyto_nuc.columns = ['total_transcripts', 'nuclear_transcripts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d359f-684e-4676-83ca-1e1b9201e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyto_nuc = cyto_nuc.fillna(0).astype(int)\n",
    "tc.index = tc.transcript_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88861d-5f6e-4e36-835d-351fd246da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell_values = t_.cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae915-4f3f-40ec-9c59-32c7fbeb7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_values.fillna('Not_assigned-0', inplace=True)\n",
    "t_['cell_number'] = cell_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1fa94-5e05-41a1-9f11-a360c6fbebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = t_[t_.overlaps_nucleus == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcac39-146b-49b8-a6dc-06d98cbfc85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_associated = tc.loc[overlap.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cdfe8-53dd-4714-973c-06777274e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "overlap['associated_nucleus'] = nuclei_associated.cell_id.values \n",
    "cell_numbers = overlap.cell_number.values\n",
    "associated_nuclei = overlap.associated_nucleus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe222e-33cb-439a-8dfb-8684e59e67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the most common value for each unique cell number\n",
    "most_common_values = {}\n",
    "\n",
    "# Iterate through the pairs of cell numbers and associated nuclei\n",
    "for cell_number, nucleus in tqdm(zip(cell_numbers, associated_nuclei)):\n",
    "    if cell_number not in most_common_values:\n",
    "        most_common_values[cell_number] = Counter()\n",
    "\n",
    "    most_common_values[cell_number][nucleus] += 1\n",
    "\n",
    "# Calculate the most common nucleus for each unique cell number\n",
    "result = {cell_number: counter.most_common(1)[0][0] for cell_number, counter in most_common_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35780ec8-1386-43ee-996e-e9e3b4b323cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(result.keys())\n",
    "values = list(result.values())\n",
    "index = [i for i in range(len(keys))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef53cd-9141-4ea8-a388-5ebd96c303a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_per = []\n",
    "for cell_number, counter in most_common_values.items():\n",
    "    nuclei_per.append(len(counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71fac-f785-42fc-85d6-02a30ee1d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap = t_[t_.overlaps_nucleus == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855e346-8491-46d4-a921-576738fb8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = len(np.unique(non_overlap.cell.values)) - len(set(np.unique(non_overlap.cell.values)).intersection(set(np.unique(overlap.cell.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47054e8-c19d-4520-8e52-0a5e3517062d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(zeros):\n",
    "    nuclei_per.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbdff5-6da6-415a-bf42-d9d028cd1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.clip(nuclei_per, 0, 10), bins=20)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel('Number of Cellpose Nuclei')\n",
    "plt.ylabel('Number of Baysor Cells')\n",
    "plt.xticks([i for i in range(10)])\n",
    "plt.title('D6')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4ffa8-f712-46c2-ac11-ffa01c4061bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_subset = t.iloc[[i for i in range(1000000)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0450efb-8383-44ce-bd84-37d53dd02646",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sub = t_subset[t_subset.overlaps_nucleus == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adcf5b-eb97-44f3-a412-7ee4f6ef1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sub_ = t_sub.dropna(subset=['cell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f969ce-c660-4019-a043-04fd42dcbb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unique_values = np.unique(t_sub_.cell.values)\n",
    "\n",
    "# import random\n",
    "# color_map = {value: (random.random(), random.random(), random.random()) for value in tqdm(unique_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d860844-654c-4a6e-89be-8f0422ed81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors1 = [color_map[value] for value in tqdm(t_sub_.cell.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f27b7d-7917-4d17-97bb-a9557b7c245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(t_sub.x.values[:3000], t_sub.y.values[:3000], s=1, c=colors[:3000])\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcb435-7eb7-4051-bdd5-f5150fd554a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(t_.x.values[:3000], t_.y.values[:3000], s=1, c=t_.overlaps_nucleus.values[:3000])\n",
    "\n",
    "# plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd7268-0c34-48fd-bc2a-315bb7cfec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_sub = t_subset.merge(tc, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f67da-9cbe-4c8d-b326-7b1aeef92d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sub2 = tc_sub[tc_sub.overlaps_nucleus_y ==1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7e83b-a875-4687-8cd2-52bb911c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = np.unique(ts_sub2.cell_id.values)\n",
    "\n",
    "# import random\n",
    "# color_map = {value: (random.random(), random.random(), random.random()) for value in tqdm(unique_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f15a17-286e-4549-9db7-e798c997da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [color_map[value] for value in tqdm(ts_sub2.cell_id.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd2ed3-2178-4508-96c1-53b44819043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "# ax1.scatter(ts_sub2.x_location.values[:3000], ts_sub2.y_location.values[:3000], s=1, c=colors[:3000])\n",
    "# ax2.scatter(t_sub_.x.values[:3000], t_sub_.y.values[:3000], s=1, c=colors1[:3000])\n",
    "# ax1.axis('equal')\n",
    "# ax2.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d555a-3b10-4a36-a551-1a412fdedc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the most common value for each unique cell number\n",
    "most_common_values = {}\n",
    "\n",
    "# Iterate through the pairs of cell numbers and associated nuclei\n",
    "for cell_number, nucleus in tqdm(zip(ts_sub2.cell.values, ts_sub2.cell_id.values)):\n",
    "    if cell_number not in most_common_values:\n",
    "        most_common_values[cell_number] = Counter()\n",
    "\n",
    "    most_common_values[cell_number][nucleus] += 1\n",
    "\n",
    "# Calculate the most common nucleus for each unique cell number\n",
    "result = {cell_number: counter.most_common(1)[0][0] for cell_number, counter in most_common_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b59f0-4d15-4c38-a6e9-8f18e7b880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "# Example clusterings (replace with your own data)\n",
    "predicted = [result.get(i) for i in ts_sub2.cell.values[:20000]]\n",
    "real = ts_sub2.cell_id.values[:20000]\n",
    "\n",
    "ami = adjusted_mutual_info_score(predicted, real)\n",
    "print(\"Adjusted Mutual Information:\", ami)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92381a49-a927-46b1-8905-6a0743020a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_d6 = sc.read(r'D:\\amonell\\timecourse\\output-XETG00095__0011274__SI_d6__20230825__004851\\adatas\\preprocessed_and_filtered_02.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a953ae5-8d8e-4c3d-9cbb-a5977b308f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = np.unique(tc.cell_id.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13040d-9939-4846-b5b5-54f1d8e8b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = np.unique(transcripts_nucleus.cell.values, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88f10b-4f5e-4a3f-8b4a-a11d5bb9ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt[0][np.argmax(rt[1][1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67f2e3-1b46-4ead-9cb6-49637cd0012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol_d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19ee7e-08bb-4e28-917a-fa382651fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(rt[1] > 20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd6735-9516-4218-8fb4-6a85afc9cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rt[1][1:], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415112c-da3d-4567-88ac-581304048e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rc[1][1:], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65f4b5-7af4-4d84-80d2-8e9a23076c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(ol_d6.obs['transcript_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a9235-2128-4180-81f9-c42e1c77ad46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.unique(ts_sub2.cell_id, return_counts=True)[1], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb90f7-f418-4c97-9f24-916217ae493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = sc.read(os.path.join(input_folders[1], 'adatas', 'preprocessed_01.h5ad')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac86ba-f687-4155-b5a3-1e85a3445880",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd341d-d30f-4a7f-88b8-a42175eff7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(experiment.obs.nuclear_transcripts.values > 20)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a5984-4b7b-496a-9402-3c12d43f584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(experiment.obs.nuclear_transcripts.values, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cf937-9fe3-4042-8ee7-a32f996e0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = input_folders[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5727765-a2ba-43ad-b6ee-c43546331f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    transcripts_cellpose = pd.read_csv(os.path.join(input_file, 'transcripts_cellpose.csv'), index_col=0)\n",
    "    transcripts = pd.read_csv(os.path.join(input_file, 'transcripts.csv'), index_col=0)\n",
    "    \n",
    "    transcripts = transcripts.dropna(subset=['cell'])\n",
    "    cell_by_gene = transcripts.groupby(['cell', 'gene']).size().unstack(fill_value=0)\n",
    "    transcripts_nucleus = transcripts[transcripts['overlaps_nucleus'] == 1]\n",
    "    cell_by_gene_nucleus = transcripts_nucleus.groupby(['cell', 'gene']).size().unstack(fill_value=0)\n",
    "    cell_by_gene_tpc = np.sum(cell_by_gene, axis=1)\n",
    "    cell_by_gene_nucleus_tpc = np.sum(cell_by_gene_nucleus, axis=1)\n",
    "    cyto_nuc = pd.DataFrame(cell_by_gene_tpc).merge(pd.DataFrame(cell_by_gene_nucleus_tpc), how='outer', left_index=True, right_index=True)\n",
    "    cyto_nuc.columns = ['total_transcripts', 'nuclear_transcripts']\n",
    "    \n",
    "    cyto_nuc = cyto_nuc.fillna(0).astype(int)\n",
    "    transcripts_cellpose.index = transcripts_cellpose.transcript_id.values\n",
    "    cell_values = transcripts.cell\n",
    "    cell_values.fillna('Not_assigned-0', inplace=True)\n",
    "    transcripts['cell_number'] = cell_values\n",
    "    overlap = transcripts[transcripts.overlaps_nucleus == 1]\n",
    "    nuclei_associated = transcripts_cellpose.loc[overlap.index.values]\n",
    "    overlap['associated_nucleus'] = nuclei_associated.cell_id.values \n",
    "    cell_numbers = overlap.cell_number.values\n",
    "    associated_nuclei = overlap.associated_nucleus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5184f-9c91-4eb9-9bb9-b9c26a440f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # Create a dictionary to store the most common value for each unique cell number\n",
    "    most_common_values = {}\n",
    "\n",
    "    # Iterate through the pairs of cell numbers and associated nuclei\n",
    "    for cell_number, nucleus in tqdm(zip(cell_numbers, associated_nuclei)):\n",
    "        if cell_number not in most_common_values:\n",
    "            most_common_values[cell_number] = Counter()\n",
    "    \n",
    "        most_common_values[cell_number][nucleus] += 1\n",
    "    \n",
    "    # Calculate the most common nucleus for each unique cell number\n",
    "    result = {cell_number: counter.most_common(1)[0][0] for cell_number, counter in most_common_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358df2c5-7c5c-4bce-a639-f2683a75d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "    keys = list(result.keys())\n",
    "    values = list(result.values())\n",
    "    index = [i for i in range(len(keys))]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc68fd3-0616-40c5-8407-0778d3b08eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    keydf = pd.DataFrame(zip(keys, values, index), columns=['cell_number', 'nucleus', 'inds'])\n",
    "    merge_dic = cyto_nuc.merge(keydf, left_index=True, right_on='cell_number', how='left')\n",
    "    merge_dic['inds'] = [i for i in range(len(merge_dic.index))]\n",
    "    groupby_most_common_nucleus = merge_dic.groupby('nucleus')\n",
    "\n",
    "    new_cyto_nuc = []\n",
    "    new_cell_by_gene = []\n",
    "    names = []\n",
    "    for group_name, group_data in tqdm(groupby_most_common_nucleus):\n",
    "        indices = group_data.inds.values\n",
    "        names.append(group_data.cell_number.values[0])\n",
    "        new_cyto_nuc.append(np.sum(cyto_nuc.iloc[indices].values, axis=0))\n",
    "        new_cell_by_gene.append(np.sum(cell_by_gene.iloc[indices].values, axis=0))\n",
    "\n",
    "    new_cell_by_gene = np.array(new_cell_by_gene)\n",
    "    new_cyto_nuc = np.array(new_cyto_nuc) \n",
    "    new_cell_by_gene = pd.DataFrame(new_cell_by_gene, columns=cell_by_gene.columns, index=names)\n",
    "    new_cyto_nuc=pd.DataFrame(new_cyto_nuc, index=names, columns=cyto_nuc.columns)\n",
    "\n",
    "    anndata = sc.AnnData(new_cell_by_gene.values, var=pd.DataFrame(index=new_cell_by_gene.columns), obs=new_cyto_nuc)\n",
    "\n",
    "    anndata.layers['raw'] = anndata.X\n",
    "    anndata.obs['cytoplasmic_transcripts'] = anndata.obs['total_transcripts'] - anndata.obs['nuclear_transcripts'] \n",
    "    anndata.obs['nuclear_transcript_percentage'] = anndata.obs['nuclear_transcripts']/anndata.obs['total_transcripts']\n",
    "    anndata.var['gene'] = anndata.var.index.values\n",
    "    anndata.obs['cell'] = anndata.obs.index.values\n",
    "    cell_spatial = transcripts.groupby('cell')[['x', 'y']].mean()\n",
    "    anndata.obs = anndata.obs.merge(cell_spatial, how='left', left_index=True, right_index=True)\n",
    "    anndata.obsm['X_spatial'] = anndata.obs[['x', 'y']].values\n",
    "    anndata = anndata[:, ~((anndata.var.index.str.contains('BLANK')) | (anndata.var.index.str.contains('NegControl')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194220c3-253a-497c-bc2a-e21387aeff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "anndata[anndata.obs['nuclear_transcripts'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in tqdm(input_folders):\n",
    "    experiment = sc.read(os.path.join(input_file, 'adatas', 'preprocessed_01.h5ad')) \n",
    "    try:\n",
    "        df = pd.DataFrame(experiment.X.A, columns=experiment.var.index.values, index=experiment.obs.index.values)\n",
    "    except:\n",
    "        df = pd.DataFrame(experiment.X, columns=experiment.var.index.values, index=experiment.obs.index.values)\n",
    "\n",
    "    metadata = experiment.obs\n",
    "    print('QC metrics for batch '+os.path.basename(input_file))\n",
    "\n",
    "    plot_qc_feature(df, metadata, False)\n",
    "\n",
    "    default_parameters = input('Do you want to use default filtering cutoffs (y/n)?')\n",
    "\n",
    "    if default_parameters == 'n':\n",
    "\n",
    "        min_transcript_threshold = float(input(\"Min transcripts threshold: \"))\n",
    "        max_transcript_threshold = float(input(\"Max transcripts threshold: \"))\n",
    "    \n",
    "        min_nuclear_transcripts = float(input(\"Min nuclear transcripts: \"))\n",
    "        max_nuclear_transcripts = float(input(\"Max nuclear transcripts: \"))\n",
    "    \n",
    "        min_cyto_transcripts = float(input(\"Min cyto transcripts: \"))\n",
    "        max_cyto_transcripts = float(input(\"Max cyto transcripts: \"))\n",
    "    \n",
    "        min_nuc_pct = float(input(\"Min nuclear transcripts / total transcripts: \"))\n",
    "        max_nuc_pct = float(input(\"Max nuclear transcripts / total transcripts: \"))\n",
    "\n",
    "        experiment = qc_before_clustering(experiment, min_transcript_threshold, max_transcript_threshold, min_nuclear_transcripts, max_nuclear_transcripts, min_cyto_transcripts, max_cyto_transcripts, min_nuc_pct, max_nuc_pct)\n",
    "    else:\n",
    "        experiment = qc_before_clustering(experiment)\n",
    "    experiment.write(os.path.join(input_file, 'adatas', 'preprocessed_and_filtered_02.h5ad'))\n",
    "    #Add lines to save out figures into new analysis subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44f095-9358-4cf7-97e0-0206fe5b3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in tqdm(input_folders):\n",
    "    experiment = sc.read(os.path.join(input_file, 'adatas', 'preprocessed_and_filtered_02.h5ad'))\n",
    "    sc.tl.pca(experiment)\n",
    "    sc.pp.neighbors(experiment)\n",
    "    sc.tl.leiden(experiment, key_added='original_leiden')\n",
    "    sc.tl.umap(experiment)\n",
    "    experiment.write(os.path.join(input_file, 'adatas', 'initial_umap_calculated_03.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45f346-1c37-4098-8308-886ed6b142e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
