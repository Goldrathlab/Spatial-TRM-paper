{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely\n",
    "import glob\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from core_functions.unrolling import *\n",
    "from core_functions.initial_neighborhoods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script performs several calculations, to get the crypt villus axis. These include calculating neighborhoods, parsing image labels on the human data to create training data, and making crypt villus axis predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_crypt_villi_axis(adata, data_dir):\n",
    "    unique_categories = np.unique(adata.obs[\"batch\"])\n",
    "\n",
    "    subset_ads = []\n",
    "    batch_ctr = 0\n",
    "    for input_file in unique_categories:\n",
    "        json_file_path = os.path.join(data_dir, input_file, \"label_img.json\")\n",
    "        downsized_adata = sc.read(\n",
    "            os.path.join(data_dir, input_file, \"adatas\", \"03_intial_neighborhoods.h5ad\")\n",
    "        )\n",
    "\n",
    "        batch_adata = adata[adata.obs[\"batch\"] == input_file]\n",
    "\n",
    "        all_spatial = batch_adata.obsm[\"X_spatial\"]\n",
    "        print(json_file_path)\n",
    "        # Load the JSON data from the file\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        # Extract relevant information from the JSON data\n",
    "        image_height = data[\"imageHeight\"]\n",
    "        image_width = data[\"imageWidth\"]\n",
    "        image_path = data[\"imagePath\"]\n",
    "        shapes = data[\"shapes\"]\n",
    "\n",
    "        for shape in shapes:\n",
    "            label = shape[\"label\"]\n",
    "            if label == \"bottom_keypoint\":\n",
    "                poly = shapely.Polygon(\n",
    "                    np.array(shape[\"points\"])\n",
    "                    * downsized_adata.uns[\"unrolling_downsize\"]\n",
    "                )\n",
    "                x = np.array(poly.boundary.xy[0])\n",
    "                y = np.array(poly.boundary.xy[1])\n",
    "        bottom_points = np.array([x, y]).T\n",
    "\n",
    "        # Process the shapes (annotations)\n",
    "        villis = []\n",
    "        peyers = []\n",
    "        villus_ct = int(0 + 1000 * batch_ctr)\n",
    "        for shape in tqdm(shapes):\n",
    "            label = shape[\"label\"]\n",
    "            if (label == \"villlus\") or (label == \"villus\"):\n",
    "                poly = shapely.Polygon(\n",
    "                    np.array(shape[\"points\"])\n",
    "                    * downsized_adata.uns[\"unrolling_downsize\"]\n",
    "                )\n",
    "                indices = []\n",
    "                for i in range(len(all_spatial)):\n",
    "                    pt = shapely.Point(all_spatial[i])\n",
    "                    if pt.within(poly):\n",
    "                        indices.append(i)\n",
    "                villis.append(indices)\n",
    "                villus_ct += 1\n",
    "            elif label == \"peyers\":\n",
    "                peyers.append(shape[\"points\"])\n",
    "\n",
    "            def find_closest_point(target_point, point_array):\n",
    "                distances = np.linalg.norm(point_array - target_point, axis=1)\n",
    "                closest_index = np.argmin(distances)\n",
    "                return point_array[closest_index], np.min(distances)\n",
    "\n",
    "        total_indices = []\n",
    "        for ir in peyers:\n",
    "            ir_ = np.array(ir) * downsized_adata.uns[\"unrolling_downsize\"]\n",
    "            poly = shapely.Polygon(ir_)\n",
    "            indices = []\n",
    "            for i in tqdm(range(len(all_spatial))):\n",
    "                pt = shapely.Point(all_spatial[i])\n",
    "                if pt.within(poly):\n",
    "                    indices.append(i)\n",
    "            total_indices.append(indices)\n",
    "\n",
    "        total_indices = list(\n",
    "            set([element for sublist in total_indices for element in sublist])\n",
    "        )\n",
    "\n",
    "        peyers = np.zeros(len(batch_adata.obs.index))\n",
    "        peyers[total_indices] = 1\n",
    "        batch_adata.obs[\"peyers\"] = peyers\n",
    "\n",
    "        villi_bottoms = []\n",
    "        for i, e in enumerate(villis):\n",
    "            points = batch_adata.obsm[\"X_spatial\"][e]\n",
    "            closest_points = []\n",
    "            distance = []\n",
    "            for point in points:\n",
    "                closest_point, dt = find_closest_point(point, bottom_points)\n",
    "                closest_points.append(closest_point)\n",
    "                distance.append(dt)\n",
    "            # Find the overall closest point\n",
    "            villi_bottoms.append(closest_points[np.argmin(distance)])\n",
    "\n",
    "        def euclidean_distance(point1, point2):\n",
    "            return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "        def distances_to_reference(array, reference_point):\n",
    "            return [euclidean_distance(point, reference_point) for point in array]\n",
    "\n",
    "        normalized_crypt_villi = np.zeros(len(batch_adata.obs.index))\n",
    "        for i, e in enumerate(villis):\n",
    "            reference_point = np.array(villi_bottoms[i])\n",
    "            array = batch_adata.obsm[\"X_spatial\"][e]\n",
    "            distances = distances_to_reference(array, reference_point)\n",
    "            distances = distances / max(distances)\n",
    "            normalized_crypt_villi[e] = distances\n",
    "\n",
    "        batch_adata.obs[\"reference_crypt_villi\"] = normalized_crypt_villi\n",
    "\n",
    "        villi_number = np.zeros(len(batch_adata.obs.index))\n",
    "        for i, e in enumerate(villis):\n",
    "            villi_number[e] = i + (1000 * batch_ctr)\n",
    "        batch_adata.obs[\"villi_number\"] = villi_number\n",
    "\n",
    "        sc.pl.embedding(\n",
    "            batch_adata,\n",
    "            basis=\"spatial\",\n",
    "            color=[\"villi_number\", \"reference_crypt_villi\", \"peyers\"],\n",
    "        )\n",
    "        batch_ctr += 1\n",
    "        subset_ads.append(batch_adata)\n",
    "\n",
    "    new_crypt_villus = np.zeros(len(adata.obs))\n",
    "    new_villus_number = np.zeros(len(adata.obs))\n",
    "    new_peyers = np.zeros(len(adata.obs))\n",
    "\n",
    "    b_count = 0\n",
    "    for input_file in unique_categories:\n",
    "        actual_adata = np.where(adata.obs[\"batch\"] == input_file)[0]\n",
    "        new_crypt_villus[actual_adata] = subset_ads[b_count].obs[\n",
    "            \"reference_crypt_villi\"\n",
    "        ]\n",
    "        new_villus_number[actual_adata] = subset_ads[b_count].obs[\"villi_number\"]\n",
    "        new_peyers[actual_adata] = subset_ads[b_count].obs[\"peyers\"]\n",
    "        b_count += 1\n",
    "\n",
    "    adata.obs[\"reference_crypt_villi\"] = new_crypt_villus\n",
    "    adata.obs[\"villi_number\"] = new_villus_number\n",
    "    adata.obs[\"peyers\"] = new_peyers\n",
    "    return adata\n",
    "\n",
    "\n",
    "def train_model(adata_in_villi, unchanging_type_keys, n_neighborhoods=6):\n",
    "    combined_adata_no_immune = adata_in_villi[\n",
    "        adata_in_villi.obs[\"Class\"].isin(unchanging_type_keys)\n",
    "        & (adata_in_villi.obs[\"peyers\"].values.astype(int) == 0)\n",
    "    ]\n",
    "    unique_batches = np.unique(combined_adata_no_immune.obs.batch.values)\n",
    "\n",
    "    nneighbors = 30\n",
    "    dfs = []\n",
    "    for input_file in unique_batches:\n",
    "        adata = combined_adata_no_immune[\n",
    "            combined_adata_no_immune.obs[\"batch\"] == input_file\n",
    "        ]\n",
    "        adata_arr = np.array(adata.X)\n",
    "        celltype_cluster = adata_in_villi.obs.index.values\n",
    "        list_of_arrays = []\n",
    "        spatial_points = np.array(\n",
    "            [\n",
    "                adata_in_villi.obsm[\"X_spatial\"][:, 0],\n",
    "                adata_in_villi.obsm[\"X_spatial\"][:, 1],\n",
    "            ]\n",
    "        ).T\n",
    "        spatial_points_ref = np.array(\n",
    "            [adata.obsm[\"X_spatial\"][:, 0], adata.obsm[\"X_spatial\"][:, 1]]\n",
    "        ).T\n",
    "        tree = KDTree(spatial_points_ref)\n",
    "        for i_bac in tqdm(range(len(celltype_cluster))):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "\n",
    "        X = pd.DataFrame(np.array(list_of_arrays))\n",
    "        dfs.append(X)\n",
    "\n",
    "    X_arr = pd.concat(dfs)\n",
    "\n",
    "    num_neighborhoods = n_neighborhoods\n",
    "\n",
    "    f = len(X.columns)\n",
    "    n = len(X.index.tolist())\n",
    "\n",
    "    model = NMF(n_components=num_neighborhoods, random_state=0)\n",
    "    W = model.fit_transform(X)\n",
    "    H = model.components_\n",
    "\n",
    "    return model, unique_batches, H\n",
    "\n",
    "\n",
    "def create_test_data(\n",
    "    original_adata, unique_batches, model, unchanging_type_keys, input_folders, H\n",
    "):\n",
    "    for batch in unique_batches:\n",
    "        adata = original_adata[original_adata.obs[\"batch\"] == batch]\n",
    "\n",
    "        superclusters = adata.obs[\"Class\"].values\n",
    "        celltype_cluster = adata.obs.index.values\n",
    "\n",
    "        dir_dictionary = {}\n",
    "        for i in np.unique(celltype_cluster):\n",
    "            dir_dictionary[i] = 0\n",
    "\n",
    "        nneighbors = 30\n",
    "        list_of_arrays = []\n",
    "        adata_epi = adata[\n",
    "            adata.obs[\"Class\"].isin(unchanging_type_keys)\n",
    "            & (adata.obs[\"peyers\"].values.astype(int) == 0)\n",
    "        ]\n",
    "        # print(np.shape(adata_epi))\n",
    "        spatial_points_epi = np.array(\n",
    "            [adata_epi.obsm[\"X_spatial\"][:, 0], adata_epi.obsm[\"X_spatial\"][:, 1]]\n",
    "        ).T\n",
    "        spatial_points = np.array(\n",
    "            [adata.obsm[\"X_spatial\"][:, 0], adata.obsm[\"X_spatial\"][:, 1]]\n",
    "        ).T\n",
    "        adata_epi_arr = np.array(adata_epi.X)\n",
    "\n",
    "        tree = KDTree(spatial_points_epi)\n",
    "        for i_bac in range(len(celltype_cluster)):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_epi_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "\n",
    "        X = pd.DataFrame(np.array(list_of_arrays)).astype(H.dtype)\n",
    "        W = model.transform(X)\n",
    "\n",
    "        topics_frame = pd.DataFrame(W)\n",
    "\n",
    "        topics_frame.columns = [\n",
    "            \"Topic \" + str(i + 1) for i in range(len(topics_frame.columns))\n",
    "        ]\n",
    "        topics_frame.index = adata.obs.index.tolist()\n",
    "\n",
    "        def zscore(column):\n",
    "            return (column - column.mean()) / column.std()\n",
    "\n",
    "        # Apply the z-score function to each column in the dataframe\n",
    "        topics_frame = topics_frame.apply(zscore)\n",
    "        adata.obs = adata.obs.drop(\n",
    "            adata.obs.columns[adata.obs.columns.str.contains(\"Topic\")], axis=1\n",
    "        )\n",
    "        adata.obs = adata.obs.merge(topics_frame, left_index=True, right_index=True)\n",
    "        adata.obs[\"topic\"] = pd.Categorical(\n",
    "            (np.argmax(topics_frame.values, axis=1) + 1).astype(str)\n",
    "        )\n",
    "\n",
    "        sc.set_figure_params(dpi=300)\n",
    "        figure = sc.pl.embedding(\n",
    "            adata,\n",
    "            basis=\"spatial\",\n",
    "            color=\"topic\",\n",
    "            vmax=1,\n",
    "            cmap=\"Blues\",\n",
    "            title=\"Neighborhood\",\n",
    "            size=2,\n",
    "            show=False,\n",
    "            return_fig=True,\n",
    "        )\n",
    "        try:\n",
    "            os.mkdir(\n",
    "                os.path.join(\n",
    "                    os.path.dirname(input_folders[0]), batch, \"figures\", \"neighborhoods\"\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            print(\"Figures/neighborhoods already made.\")\n",
    "        figure.tight_layout()\n",
    "        plt.axis(\"equal\")\n",
    "        figure.savefig(\n",
    "            os.path.join(\n",
    "                os.path.dirname(input_folders[0]),\n",
    "                batch,\n",
    "                \"figures\",\n",
    "                \"neighborhoods\",\n",
    "                \"neighborhoods.png\",\n",
    "            )\n",
    "        )\n",
    "        plt.close()\n",
    "        adata.write(\n",
    "            os.path.join(\n",
    "                os.path.dirname(input_folders[0]),\n",
    "                batch,\n",
    "                \"adatas\",\n",
    "                \"05_before_decomposition_model.h5ad\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def create_training_data(adata_subset, model, H, unchanging_type_keys):\n",
    "    unique_batches = np.unique(adata_subset.obs[\"batch\"])\n",
    "    training_datas = []\n",
    "    all_train_labels = []\n",
    "    for uniq in unique_batches:\n",
    "        adata_sub_batch = adata_subset[adata_subset.obs[\"batch\"] == uniq]\n",
    "        superclusters = adata_sub_batch.obs[\"Class\"].values\n",
    "        celltype_cluster = adata_sub_batch.obs.index.values\n",
    "\n",
    "        dir_dictionary = {}\n",
    "        for i in np.unique(celltype_cluster):\n",
    "            dir_dictionary[i] = 0\n",
    "\n",
    "        nneighbors = 30\n",
    "        list_of_arrays = []\n",
    "        adata_epi = adata_sub_batch[\n",
    "            adata_sub_batch.obs[\"Class\"].isin(unchanging_type_keys)\n",
    "            & (adata_sub_batch.obs[\"peyers\"] == 0)\n",
    "        ]\n",
    "        # print(np.shape(adata_epi))\n",
    "        spatial_points_epi = np.array(\n",
    "            [adata_epi.obsm[\"X_spatial\"][:, 0], adata_epi.obsm[\"X_spatial\"][:, 1]]\n",
    "        ).T\n",
    "        spatial_points = np.array(\n",
    "            [\n",
    "                adata_sub_batch.obsm[\"X_spatial\"][:, 0],\n",
    "                adata_sub_batch.obsm[\"X_spatial\"][:, 1],\n",
    "            ]\n",
    "        ).T\n",
    "        adata_epi_arr = np.array(adata_epi.X)\n",
    "\n",
    "        tree = KDTree(spatial_points_epi)\n",
    "        for i_bac in tqdm(range(len(celltype_cluster))):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_epi_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "\n",
    "        X = pd.DataFrame(np.array(list_of_arrays)).astype(H.dtype)\n",
    "        W = model.transform(X)\n",
    "\n",
    "        topics_frame = pd.DataFrame(W)\n",
    "\n",
    "        topics_frame.columns = [\n",
    "            \"Topic \" + str(i + 1) for i in range(len(topics_frame.columns))\n",
    "        ]\n",
    "        topics_frame.index = adata_sub_batch.obs.index.tolist()\n",
    "\n",
    "        def zscore(column):\n",
    "            return (column - column.mean()) / column.std()\n",
    "\n",
    "        # Apply the z-score function to each column in the dataframe\n",
    "        topics_frame = topics_frame.apply(zscore)\n",
    "        training_datas.append(topics_frame.values)\n",
    "        all_train_labels.append(adata_sub_batch.obs[\"reference_crypt_villi\"].values)\n",
    "\n",
    "    training_data = []\n",
    "    for i in training_datas:\n",
    "        for j in i:\n",
    "            training_data.append(j)\n",
    "    training_data = np.array(training_data)\n",
    "\n",
    "    training_labels = []\n",
    "    for i in all_train_labels:\n",
    "        for j in i:\n",
    "            training_labels.append(j)\n",
    "    training_labels = np.array(training_labels)\n",
    "\n",
    "    return training_data, training_labels\n",
    "\n",
    "\n",
    "def train_neural_network(training_data, training_labels, epoch_num=15):\n",
    "    # Define model\n",
    "    neural = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(\n",
    "                64, activation=\"relu\", input_shape=(training_data.shape[1],)\n",
    "            ),\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    neural.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    neural.fit(\n",
    "        training_data, training_labels, epochs=epoch_num, batch_size=32, verbose=2\n",
    "    )\n",
    "    return neural\n",
    "\n",
    "\n",
    "def calculate_crypt_villi_axis(input_folders, neural):\n",
    "    for input_file in input_folders:\n",
    "        adata = sc.read(\n",
    "            os.path.join(input_file, \"adatas\", \"05_before_decomposition_model.h5ad\")\n",
    "        )\n",
    "        if len(adata.obs.columns[adata.obs.columns.str.contains(\"_x\")]) > 0:\n",
    "            adata.obs = adata.obs.drop(\n",
    "                adata.obs.columns[adata.obs.columns.str.contains(\"_x\")], axis=1\n",
    "            )\n",
    "        else:\n",
    "            adata.obs = adata.obs.drop(\n",
    "                adata.obs.columns[adata.obs.columns.str.contains(\"_y\")], axis=1\n",
    "            )\n",
    "\n",
    "        testing_data = adata.obs[\n",
    "            adata.obs.columns[adata.obs.columns.str.contains(\"Topic\")]\n",
    "        ].values\n",
    "        predictions = neural.predict(testing_data)\n",
    "        adata.obs[\"crypt_villi_axis\"] = predictions\n",
    "        adata.write(os.path.join(input_file, \"adatas\", \"06_axes_defined.h5ad\"))\n",
    "        fig = sc.pl.embedding(\n",
    "            adata[adata.obs.peyers == 0],\n",
    "            basis=\"spatial\",\n",
    "            color=\"crypt_villi_axis\",\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "            vmax=1,\n",
    "            cmap=\"viridis\",\n",
    "            size=4,\n",
    "        )\n",
    "        fig.tight_layout()\n",
    "        plt.axis(\"equal\")\n",
    "        fig.savefig(\n",
    "            os.path.join(input_file, \"figures\", \"axes\", f\"spatial_crypt_villi.png\")\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def calculate_epithelial_axis(input_folders):\n",
    "    sc.set_figure_params(dpi=1000, dpi_save=1000)\n",
    "    for input_file in input_folders:\n",
    "        ad = sc.read(os.path.join(input_file, \"adatas\", \"06_axes_defined.h5ad\"))\n",
    "        points_epi = ad[ad.obs.Class.isin([\"Epithelial\"])].obsm[\"X_spatial\"]\n",
    "\n",
    "        all_tree = KDTree(ad.obsm[\"X_spatial\"])\n",
    "        epi_tree = KDTree(points_epi)\n",
    "        distances_all, neighbors_all = all_tree.query(ad.obsm[\"X_spatial\"], k=5)\n",
    "        distances, neighbors = epi_tree.query(ad.obsm[\"X_spatial\"], k=5)\n",
    "        distance_medians = np.mean(distances, axis=1) / np.mean(distances_all, axis=1)\n",
    "        ad.obs[\"epithelial_distance\"] = distance_medians\n",
    "        ad.obs[\"epithelial_distance\"] = ad.obs[\"epithelial_distance\"] / np.percentile(\n",
    "            ad.obs[\"epithelial_distance\"], 99\n",
    "        )\n",
    "        fig = sc.pl.embedding(\n",
    "            ad,\n",
    "            basis=\"spatial\",\n",
    "            color=\"epithelial_distance\",\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "            vmax=1,\n",
    "            cmap=\"viridis\",\n",
    "            size=4,\n",
    "        )\n",
    "        fig.tight_layout()\n",
    "        plt.axis(\"equal\")\n",
    "        fig.savefig(\n",
    "            os.path.join(input_file, \"figures\", \"axes\", f\"spatial_epithelial.png\")\n",
    "        )\n",
    "        plt.close()\n",
    "        ad.write(os.path.join(input_file, \"adatas\", \"07_final_object.h5ad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the path to the preliminary celltyped human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r\"/mnt/sata1/Analysis_Alex/human_r1/analysis/cleaned\"\n",
    "adata_whole = sc.read(os.path.join(output_folder, \"celltyped_do_not_touch.h5ad\"))\n",
    "\n",
    "\n",
    "subjects = [[\"human_09_r1\", \"human_09_r2\"], [\"human_05_r1\", \"human_05_r2\"]]\n",
    "for sub in subjects:\n",
    "    adata = adata_whole[adata_whole.obs[\"batch\"].isin(sub)]\n",
    "\n",
    "    print()\n",
    "    unchanging_type_keys = [\"Epithelial\"]\n",
    "\n",
    "    data_dir = \"/mnt/sata1/Analysis_Alex/human_r1\"\n",
    "\n",
    "    input_folders = glob.glob(\n",
    "        os.path.join(data_dir, sub[0].split(\"_\")[0] + \"_\" + sub[0].split(\"_\")[1] + \"*\")\n",
    "    )\n",
    "\n",
    "    adata = recalculate_crypt_villi_axis(adata, data_dir)\n",
    "\n",
    "    original_adata = adata.copy()\n",
    "\n",
    "    adata_subset = adata[adata.obs[\"villi_number\"] > 0]\n",
    "\n",
    "    print(np.shape(adata_subset.obs))\n",
    "\n",
    "    model, unique_batches, H = train_model(adata_subset, unchanging_type_keys)\n",
    "\n",
    "    create_test_data(\n",
    "        original_adata, unique_batches, model, unchanging_type_keys, input_folders, H\n",
    "    )\n",
    "\n",
    "    training_data, training_labels = create_training_data(\n",
    "        adata_subset, model, H, unchanging_type_keys\n",
    "    )\n",
    "\n",
    "    neural = train_neural_network(training_data, training_labels)\n",
    "\n",
    "    calculate_crypt_villi_axis(input_folders, neural)\n",
    "\n",
    "    calculate_epithelial_axis(input_folders=input_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folders = glob.glob(os.path.join(data_dir, \"human_0*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_concat = []\n",
    "for input_file in input_folders:\n",
    "    ad = sc.read(os.path.join(input_file, \"adatas\", \"07_final_object.h5ad\"))\n",
    "    ads_concat.append(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adata = sc.concat(ads_concat, uns_merge=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adata.write(os.path.join(output_folder, \"final_human_adata.h5ad\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
