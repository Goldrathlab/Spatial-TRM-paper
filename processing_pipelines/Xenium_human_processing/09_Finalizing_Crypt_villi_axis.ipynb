{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely\n",
    "import glob\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from core_functions.unrolling import *\n",
    "from core_functions.initial_neighborhoods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script performs several calculations, to get the crypt villus axis. These include calculating neighborhoods, parsing image labels on the human data to create training data, and making crypt villus axis predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recalculate_crypt_villi_axis(adata, data_dir):\n",
    "    unique_categories = np.unique(adata.obs['batch'])\n",
    "\n",
    "    subset_ads = []\n",
    "    batch_ctr = 0\n",
    "    for input_file in unique_categories:\n",
    "        json_file_path = os.path.join(data_dir, input_file, 'label_img.json')\n",
    "        downsized_adata = sc.read(os.path.join(data_dir, input_file, 'adatas', '03_intial_neighborhoods.h5ad'))\n",
    "\n",
    "        batch_adata = adata[adata.obs['batch'] == input_file]\n",
    "\n",
    "        all_spatial = batch_adata.obsm['X_spatial']\n",
    "        print(json_file_path)\n",
    "        # Load the JSON data from the file\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Extract relevant information from the JSON data\n",
    "        image_height = data['imageHeight']\n",
    "        image_width = data['imageWidth']\n",
    "        image_path = data['imagePath']\n",
    "        shapes = data['shapes']\n",
    "\n",
    "\n",
    "        for shape in shapes:\n",
    "            label = shape['label']\n",
    "            if label == 'bottom_keypoint':\n",
    "                poly = shapely.Polygon(np.array(shape['points'])* downsized_adata.uns['unrolling_downsize'])\n",
    "                x = np.array(poly.boundary.xy[0])\n",
    "                y = np.array(poly.boundary.xy[1])\n",
    "        bottom_points = np.array([x, y]).T\n",
    "\n",
    "        # Process the shapes (annotations)\n",
    "        villis = [] \n",
    "        peyers = []\n",
    "        villus_ct = int(0 + 1000*batch_ctr)\n",
    "        for shape in tqdm(shapes):\n",
    "            label = shape['label']\n",
    "            if (label == 'villlus') or (label == 'villus'):\n",
    "                poly = shapely.Polygon(np.array(shape['points'])*downsized_adata.uns['unrolling_downsize'])\n",
    "                indices = []\n",
    "                for i in range(len(all_spatial)):\n",
    "                    pt = shapely.Point(all_spatial[i])\n",
    "                    if pt.within(poly):\n",
    "                        indices.append(i)\n",
    "                villis.append(indices)\n",
    "                villus_ct += 1\n",
    "            elif label == 'peyers':\n",
    "                peyers.append(shape['points'])\n",
    "\n",
    "            def find_closest_point(target_point, point_array):\n",
    "                distances = np.linalg.norm(point_array - target_point, axis=1)\n",
    "                closest_index = np.argmin(distances)\n",
    "                return point_array[closest_index], np.min(distances)\n",
    "\n",
    "        total_indices = []\n",
    "        for ir in peyers:\n",
    "            ir_ = np.array(ir)* downsized_adata.uns['unrolling_downsize']\n",
    "            poly = shapely.Polygon(ir_)\n",
    "            indices = []\n",
    "            for i in tqdm(range(len(all_spatial))):\n",
    "                pt = shapely.Point(all_spatial[i])\n",
    "                if pt.within(poly):\n",
    "                    indices.append(i)\n",
    "            total_indices.append(indices)\n",
    "        \n",
    "        total_indices=list(set([element for sublist in total_indices for element in sublist]))\n",
    "\n",
    "        peyers = np.zeros(len(batch_adata.obs.index))\n",
    "        peyers[total_indices] = 1\n",
    "        batch_adata.obs['peyers'] = peyers\n",
    "\n",
    "\n",
    "        villi_bottoms = []\n",
    "        for i, e in enumerate(villis):\n",
    "            points = batch_adata.obsm['X_spatial'][e]\n",
    "            closest_points = []\n",
    "            distance = []\n",
    "            for point in points:\n",
    "                closest_point, dt = find_closest_point(point, bottom_points)\n",
    "                closest_points.append(closest_point)\n",
    "                distance.append(dt)\n",
    "            # Find the overall closest point\n",
    "            villi_bottoms.append(closest_points[np.argmin(distance)])\n",
    "\n",
    "        def euclidean_distance(point1, point2):\n",
    "            return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "        def distances_to_reference(array, reference_point):\n",
    "            return [euclidean_distance(point, reference_point) for point in array]\n",
    "\n",
    "        normalized_crypt_villi = np.zeros(len(batch_adata.obs.index))\n",
    "        for i, e in enumerate(villis):\n",
    "            reference_point = np.array(villi_bottoms[i])\n",
    "            array = batch_adata.obsm['X_spatial'][e]\n",
    "            distances = distances_to_reference(array, reference_point)\n",
    "            distances = distances/max(distances)\n",
    "            normalized_crypt_villi[e] = distances\n",
    "\n",
    "        batch_adata.obs['reference_crypt_villi'] = normalized_crypt_villi\n",
    "\n",
    "        villi_number = np.zeros(len(batch_adata.obs.index))\n",
    "        for i, e in enumerate(villis):\n",
    "            villi_number[e] = i+ (1000*batch_ctr)\n",
    "        batch_adata.obs['villi_number'] = villi_number\n",
    "\n",
    "\n",
    "        sc.pl.embedding(batch_adata, basis='spatial', color=['villi_number', 'reference_crypt_villi', 'peyers'])\n",
    "        batch_ctr += 1\n",
    "        subset_ads.append(batch_adata)\n",
    "\n",
    "    new_crypt_villus = np.zeros(len(adata.obs))\n",
    "    new_villus_number = np.zeros(len(adata.obs))\n",
    "    new_peyers = np.zeros(len(adata.obs))\n",
    "\n",
    "    b_count = 0\n",
    "    for input_file in unique_categories:\n",
    "        actual_adata = np.where(adata.obs['batch'] == input_file)[0]\n",
    "        new_crypt_villus[actual_adata] = subset_ads[b_count].obs['reference_crypt_villi']\n",
    "        new_villus_number[actual_adata] = subset_ads[b_count].obs['villi_number']\n",
    "        new_peyers[actual_adata] = subset_ads[b_count].obs['peyers']\n",
    "        b_count += 1\n",
    "\n",
    "    adata.obs['reference_crypt_villi'] = new_crypt_villus\n",
    "    adata.obs['villi_number'] = new_villus_number\n",
    "    adata.obs['peyers'] = new_peyers\n",
    "    return adata\n",
    "\n",
    "def train_model(adata_in_villi, unchanging_type_keys, n_neighborhoods = 6):\n",
    "    combined_adata_no_immune = adata_in_villi[adata_in_villi.obs['Class'].isin(unchanging_type_keys) & (adata_in_villi.obs['peyers'].values.astype(int) == 0)]\n",
    "    unique_batches = np.unique(combined_adata_no_immune.obs.batch.values)\n",
    "\n",
    "    nneighbors = 30\n",
    "    dfs = []\n",
    "    for input_file in unique_batches:\n",
    "        adata = combined_adata_no_immune[combined_adata_no_immune.obs['batch'] == input_file]\n",
    "        adata_arr = np.array(adata.X)\n",
    "        celltype_cluster = adata_in_villi.obs.index.values\n",
    "        list_of_arrays = []\n",
    "        spatial_points = np.array([adata_in_villi.obsm['X_spatial'][:,0], adata_in_villi.obsm['X_spatial'][:,1]]).T\n",
    "        spatial_points_ref = np.array([adata.obsm['X_spatial'][:,0], adata.obsm['X_spatial'][:,1]]).T\n",
    "        tree = KDTree(spatial_points_ref)\n",
    "        for i_bac in tqdm(range(len(celltype_cluster))):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "        \n",
    "        X = pd.DataFrame(np.array(list_of_arrays))\n",
    "        dfs.append(X)\n",
    "\n",
    "\n",
    "    X_arr = pd.concat(dfs)\n",
    "\n",
    "    num_neighborhoods = n_neighborhoods\n",
    "\n",
    "    f = len(X.columns)\n",
    "    n = len(X.index.tolist())\n",
    "\n",
    "    model = NMF(n_components=num_neighborhoods, random_state=0)\n",
    "    W = model.fit_transform(X)\n",
    "    H = model.components_\n",
    "    \n",
    "    return model, unique_batches, H\n",
    "\n",
    "def create_test_data(original_adata, unique_batches, model, unchanging_type_keys, input_folders, H):\n",
    "    for batch in unique_batches:\n",
    "        adata = original_adata[original_adata.obs['batch'] == batch]\n",
    "        \n",
    "        superclusters = adata.obs['Class'].values\n",
    "        celltype_cluster = adata.obs.index.values\n",
    "\n",
    "        dir_dictionary = {}\n",
    "        for i in np.unique(celltype_cluster):\n",
    "            dir_dictionary[i] = 0\n",
    "\n",
    "        nneighbors = 30\n",
    "        list_of_arrays = []\n",
    "        adata_epi = adata[adata.obs['Class'].isin(unchanging_type_keys) & (adata.obs['peyers'].values.astype(int) == 0)]\n",
    "        # print(np.shape(adata_epi))\n",
    "        spatial_points_epi = np.array([adata_epi.obsm['X_spatial'][:,0], adata_epi.obsm['X_spatial'][:,1]]).T\n",
    "        spatial_points = np.array([adata.obsm['X_spatial'][:,0], adata.obsm['X_spatial'][:,1]]).T\n",
    "        adata_epi_arr = np.array(adata_epi.X)\n",
    "        \n",
    "        tree = KDTree(spatial_points_epi)\n",
    "        for i_bac in range(len(celltype_cluster)):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_epi_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "        \n",
    "        X = pd.DataFrame(np.array(list_of_arrays)).astype(H.dtype)\n",
    "        W = model.transform(X)\n",
    "        \n",
    "        topics_frame = pd.DataFrame(W)\n",
    "        \n",
    "        topics_frame.columns = ['Topic '+str(i+1) for i in range(len(topics_frame.columns))]\n",
    "        topics_frame.index = adata.obs.index.tolist()\n",
    "        def zscore(column):\n",
    "            return (column - column.mean()) / column.std()\n",
    "        \n",
    "        # Apply the z-score function to each column in the dataframe\n",
    "        topics_frame = topics_frame.apply(zscore)\n",
    "        adata.obs = adata.obs.drop(adata.obs.columns[adata.obs.columns.str.contains('Topic')], axis=1)\n",
    "        adata.obs=adata.obs.merge(topics_frame, left_index=True, right_index=True)\n",
    "        adata.obs['topic'] = pd.Categorical((np.argmax(topics_frame.values, axis = 1)+1).astype(str))\n",
    "\n",
    "        sc.set_figure_params(dpi=300)\n",
    "        figure = sc.pl.embedding(adata, basis='spatial', color='topic', vmax=1, cmap='Blues', title='Neighborhood', size=2, show=False, return_fig=True)\n",
    "        try:\n",
    "            os.mkdir(os.path.join(os.path.dirname(input_folders[0]), batch,'figures', 'neighborhoods'))\n",
    "        except:\n",
    "            print('Figures/neighborhoods already made.')\n",
    "        figure.tight_layout()\n",
    "        plt.axis('equal')\n",
    "        figure.savefig(os.path.join(os.path.dirname(input_folders[0]), batch,'figures', 'neighborhoods', 'neighborhoods.png'))\n",
    "        plt.close()\n",
    "        adata.write(os.path.join(os.path.dirname(input_folders[0]), batch, 'adatas', '05_before_decomposition_model.h5ad'))\n",
    "\n",
    "\n",
    "def create_training_data(adata_subset, model, H, unchanging_type_keys):\n",
    "    unique_batches = np.unique(adata_subset.obs['batch'])\n",
    "    training_datas = []\n",
    "    all_train_labels = []\n",
    "    for uniq in unique_batches:\n",
    "        adata_sub_batch = adata_subset[adata_subset.obs['batch'] == uniq]\n",
    "        superclusters = adata_sub_batch.obs['Class'].values\n",
    "        celltype_cluster = adata_sub_batch.obs.index.values\n",
    "\n",
    "        dir_dictionary = {}\n",
    "        for i in np.unique(celltype_cluster):\n",
    "            dir_dictionary[i] = 0\n",
    "\n",
    "        nneighbors = 30\n",
    "        list_of_arrays = []\n",
    "        adata_epi = adata_sub_batch[adata_sub_batch.obs['Class'].isin(unchanging_type_keys) & (adata_sub_batch.obs['peyers'] == 0)]\n",
    "        # print(np.shape(adata_epi))\n",
    "        spatial_points_epi = np.array([adata_epi.obsm['X_spatial'][:,0], adata_epi.obsm['X_spatial'][:,1]]).T\n",
    "        spatial_points = np.array([adata_sub_batch.obsm['X_spatial'][:,0], adata_sub_batch.obsm['X_spatial'][:,1]]).T\n",
    "        adata_epi_arr = np.array(adata_epi.X)\n",
    "\n",
    "        tree = KDTree(spatial_points_epi)\n",
    "        for i_bac in tqdm(range(len(celltype_cluster))):\n",
    "            current_cell = celltype_cluster[i_bac]\n",
    "            distances, neighbors = tree.query(spatial_points[i_bac], k=nneighbors)\n",
    "            neighbors = np.array(list(neighbors))\n",
    "            gene_array = np.array(np.sum(adata_epi_arr[neighbors, :], axis=0)).squeeze()\n",
    "            list_of_arrays.append(gene_array)\n",
    "\n",
    "        X = pd.DataFrame(np.array(list_of_arrays)).astype(H.dtype)\n",
    "        W = model.transform(X)\n",
    "\n",
    "        topics_frame = pd.DataFrame(W)\n",
    "\n",
    "        topics_frame.columns = ['Topic '+str(i+1) for i in range(len(topics_frame.columns))]\n",
    "        topics_frame.index = adata_sub_batch.obs.index.tolist()\n",
    "        def zscore(column):\n",
    "            return (column - column.mean()) / column.std()\n",
    "\n",
    "        # Apply the z-score function to each column in the dataframe\n",
    "        topics_frame = topics_frame.apply(zscore)\n",
    "        training_datas.append(topics_frame.values)\n",
    "        all_train_labels.append(adata_sub_batch.obs['reference_crypt_villi'].values)\n",
    "\n",
    "\n",
    "    training_data = []\n",
    "    for i in training_datas:\n",
    "        for j in i:\n",
    "            training_data.append(j)\n",
    "    training_data = np.array(training_data)  \n",
    "\n",
    "    training_labels = []\n",
    "    for i in all_train_labels:\n",
    "        for j in i:\n",
    "            training_labels.append(j)\n",
    "    training_labels = np.array(training_labels) \n",
    "\n",
    "    return training_data, training_labels\n",
    "\n",
    "def train_neural_network(training_data, training_labels, epoch_num = 15):\n",
    "    # Define model\n",
    "    neural = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(training_data.shape[1],)),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    neural.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    neural.fit(training_data, training_labels, epochs=epoch_num, batch_size=32, verbose= 2)\n",
    "    return neural\n",
    "\n",
    "def calculate_crypt_villi_axis(input_folders, neural):\n",
    "    for input_file in input_folders:\n",
    "        adata = sc.read(os.path.join(input_file,'adatas', '05_before_decomposition_model.h5ad'))\n",
    "        if len(adata.obs.columns[adata.obs.columns.str.contains('_x')]) > 0:\n",
    "            adata.obs = adata.obs.drop(adata.obs.columns[adata.obs.columns.str.contains('_x')], axis=1)\n",
    "        else:\n",
    "            adata.obs = adata.obs.drop(adata.obs.columns[adata.obs.columns.str.contains('_y')], axis=1)\n",
    "\n",
    "        testing_data = adata.obs[adata.obs.columns[adata.obs.columns.str.contains('Topic')]].values\n",
    "        predictions = neural.predict(testing_data)\n",
    "        adata.obs['crypt_villi_axis'] = predictions  \n",
    "        adata.write(os.path.join(input_file, 'adatas', '06_axes_defined.h5ad'))\n",
    "        fig = sc.pl.embedding(adata[adata.obs.peyers==0], basis = 'spatial', color='crypt_villi_axis', return_fig=True, show=False, vmax=1, cmap='viridis', size=4)\n",
    "        fig.tight_layout()\n",
    "        plt.axis('equal')\n",
    "        fig.savefig(os.path.join(input_file, 'figures', 'axes', f'spatial_crypt_villi.png'))\n",
    "        plt.close()\n",
    "\n",
    "def calculate_epithelial_axis(input_folders):\n",
    "    sc.set_figure_params(dpi=1000, dpi_save=1000)\n",
    "    for input_file in input_folders:\n",
    "        ad = sc.read(os.path.join(input_file, 'adatas', '06_axes_defined.h5ad')) \n",
    "        points_epi = ad[ad.obs.Class.isin(['Epithelial'])].obsm['X_spatial']\n",
    "\n",
    "        all_tree = KDTree(ad.obsm['X_spatial'])\n",
    "        epi_tree = KDTree(points_epi)\n",
    "        distances_all, neighbors_all = all_tree.query(ad.obsm['X_spatial'], k=5)\n",
    "        distances, neighbors = epi_tree.query(ad.obsm['X_spatial'], k=5)\n",
    "        distance_medians = (np.mean(distances, axis=1)/np.mean(distances_all, axis=1))\n",
    "        ad.obs['epithelial_distance'] = distance_medians\n",
    "        ad.obs['epithelial_distance'] = ad.obs['epithelial_distance']/np.percentile(ad.obs['epithelial_distance'], 99)\n",
    "        fig = sc.pl.embedding(ad, basis = 'spatial', color='epithelial_distance', return_fig=True, show=False, vmax=1, cmap='viridis', size=4)\n",
    "        fig.tight_layout()\n",
    "        plt.axis('equal')\n",
    "        fig.savefig(os.path.join(input_file, 'figures', 'axes', f'spatial_epithelial.png'))\n",
    "        plt.close()\n",
    "        ad.write(os.path.join(input_file, 'adatas', '07_final_object.h5ad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the path to the preliminary celltyped human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'/mnt/sata1/Analysis_Alex/human_r1/analysis/cleaned'\n",
    "adata_whole = sc.read(os.path.join(output_folder, 'celltyped_do_not_touch.h5ad'))\n",
    "\n",
    "\n",
    "subjects = [['human_09_r1', 'human_09_r2'], ['human_05_r1', 'human_05_r2']]\n",
    "for sub in subjects:\n",
    "    adata = adata_whole[adata_whole.obs['batch'].isin(sub)]\n",
    "\n",
    "    print()\n",
    "    unchanging_type_keys = ['Epithelial']\n",
    "\n",
    "    data_dir = '/mnt/sata1/Analysis_Alex/human_r1'\n",
    "\n",
    "    input_folders = glob.glob(os.path.join(data_dir, sub[0].split('_')[0]+'_'+sub[0].split('_')[1]+'*'))\n",
    "\n",
    "    adata = recalculate_crypt_villi_axis(adata, data_dir)\n",
    "\n",
    "    original_adata = adata.copy()\n",
    "\n",
    "    adata_subset = adata[adata.obs['villi_number'] > 0]\n",
    "\n",
    "    print(np.shape(adata_subset.obs))\n",
    "\n",
    "    model, unique_batches, H = train_model(adata_subset, unchanging_type_keys)\n",
    "\n",
    "    create_test_data(original_adata, unique_batches, model, unchanging_type_keys, input_folders, H)\n",
    "\n",
    "    training_data, training_labels = create_training_data(adata_subset, model, H, unchanging_type_keys)\n",
    "\n",
    "    neural = train_neural_network(training_data, training_labels)\n",
    "\n",
    "    calculate_crypt_villi_axis(input_folders, neural)\n",
    "\n",
    "    calculate_epithelial_axis(input_folders=input_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folders = glob.glob(os.path.join(data_dir, 'human_0*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amonell/.local/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/home/amonell/.local/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/home/amonell/.local/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/home/amonell/.local/lib/python3.10/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ads_concat = []\n",
    "for input_file in input_folders:\n",
    "    ad = sc.read(os.path.join(input_file, 'adatas', '07_final_object.h5ad'))\n",
    "    ads_concat.append(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adata = sc.concat(ads_concat, uns_merge='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_adata.write(os.path.join(output_folder, 'final_human_adata.h5ad'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
